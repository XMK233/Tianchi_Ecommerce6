{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e41bae6b-8702-4d6f-9c8c-2e506dfc6886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "storage dir: /mnt/d/forCoding_data/Tianchi_Ecommerce6/plan_2-あ-基于1-换roberta1\n",
      "code dir: /mnt/d/forCoding_code/Tianchi_Ecommerce6/plan_2-あ-基于1-换roberta1\n"
     ]
    }
   ],
   "source": [
    "with open(\"/mnt/d/forCoding_code/_init_tools/x__params.py\", \"r\") as f:\n",
    "    txt = f.read()\n",
    "exec(txt)\n",
    "\n",
    "with open(\"/mnt/d/forCoding_code/_init_tools/report_macro_0516.py\", \"r\") as f:\n",
    "    txt = f.read()\n",
    "exec(txt)\n",
    "\n",
    "with open(\"/mnt/d/forCoding_code/_init_tools/x__tool_here1.py\", \"r\") as f:\n",
    "    exec(f.read())\n",
    "with open(\"/mnt/d/forCoding_code/_init_tools/x__tool_here2.py\", \"r\") as f:\n",
    "    exec(f.read())\n",
    "\n",
    "wasted_dir = \"./wasted\"\n",
    "if not os.path.exists(wasted_dir):\n",
    "    os.makedirs(wasted_dir)\n",
    "\n",
    "pd.set_option('display.max_rows',200)\n",
    "pd.set_option('display.max_columns',200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df61c3f-0e69-4640-87cd-196d7285a1ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e341566f-88a3-4812-8928-a380b74ce6ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/xiuminke/miniconda3/envs/ml12/lib/python3.11/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "Starting 5-fold CV training.\n",
      "Seed: 42\n",
      "Epochs (Extract): 5\n",
      "Epochs (Classify): 5\n",
      "Output Directory: /mnt/d/forCoding_data/Tianchi_Ecommerce6/plan_2-あ-基于1-换roberta1/trained_models/cv\n",
      "Training Fold 1/5...\n",
      "Some weights of ExtractionModel were not initialized from the model checkpoint at /mnt/d/HuggingFaceModels/models--hfl--chinese-roberta-wwm-ext/ and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  Training Extraction Model (Fold: fold_0)\n",
      "    Epoch 1/5: 100%|██████████████████████████| 162/162 [00:44<00:00,  3.67it/s]\n",
      "    Loss: 0.3119\n",
      "    Epoch 2/5: 100%|██████████████████████████| 162/162 [00:43<00:00,  3.68it/s]\n",
      "    Loss: 0.1306\n",
      "    Epoch 3/5: 100%|██████████████████████████| 162/162 [00:44<00:00,  3.66it/s]\n",
      "    Loss: 0.0962\n",
      "    Epoch 4/5: 100%|██████████████████████████| 162/162 [00:44<00:00,  3.66it/s]\n",
      "    Loss: 0.0716\n",
      "    Epoch 5/5: 100%|██████████████████████████| 162/162 [00:44<00:00,  3.66it/s]\n",
      "    Loss: 0.0583\n",
      "Some weights of ClassificationModel were not initialized from the model checkpoint at /mnt/d/HuggingFaceModels/models--hfl--chinese-roberta-wwm-ext/ and are newly initialized: ['cat_classifier.bias', 'cat_classifier.weight', 'pol_classifier.bias', 'pol_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  Training Classification Model (Fold: fold_0)\n",
      "    Epoch 1/5: 100%|██████████████████████████| 569/569 [02:33<00:00,  3.71it/s]\n",
      "    Loss: 1.1222\n",
      "    Epoch 2/5: 100%|██████████████████████████| 569/569 [02:33<00:00,  3.71it/s]\n",
      "    Loss: 0.3566\n",
      "    Epoch 3/5: 100%|██████████████████████████| 569/569 [02:33<00:00,  3.71it/s]\n",
      "    Loss: 0.2152\n",
      "    Epoch 4/5: 100%|██████████████████████████| 569/569 [02:33<00:00,  3.71it/s]\n",
      "    Loss: 0.1542\n",
      "    Epoch 5/5: 100%|██████████████████████████| 569/569 [02:33<00:00,  3.71it/s]\n",
      "    Loss: 0.1191\n",
      "Training Fold 2/5...\n",
      "Some weights of ExtractionModel were not initialized from the model checkpoint at /mnt/d/HuggingFaceModels/models--hfl--chinese-roberta-wwm-ext/ and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  Training Extraction Model (Fold: fold_1)\n",
      "    Epoch 1/5: 100%|██████████████████████████| 162/162 [00:44<00:00,  3.63it/s]\n",
      "    Loss: 0.3138\n",
      "    Epoch 2/5: 100%|██████████████████████████| 162/162 [00:44<00:00,  3.65it/s]\n",
      "    Loss: 0.1306\n",
      "    Epoch 3/5: 100%|██████████████████████████| 162/162 [00:44<00:00,  3.65it/s]\n",
      "    Loss: 0.0952\n",
      "    Epoch 4/5: 100%|██████████████████████████| 162/162 [00:44<00:00,  3.65it/s]\n",
      "    Loss: 0.0709\n",
      "    Epoch 5/5: 100%|██████████████████████████| 162/162 [00:44<00:00,  3.64it/s]\n",
      "    Loss: 0.0582\n",
      "Some weights of ClassificationModel were not initialized from the model checkpoint at /mnt/d/HuggingFaceModels/models--hfl--chinese-roberta-wwm-ext/ and are newly initialized: ['cat_classifier.bias', 'cat_classifier.weight', 'pol_classifier.bias', 'pol_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  Training Classification Model (Fold: fold_1)\n",
      "    Epoch 1/5: 100%|██████████████████████████| 569/569 [02:33<00:00,  3.71it/s]\n",
      "    Loss: 1.1265\n",
      "    Epoch 2/5: 100%|██████████████████████████| 569/569 [02:33<00:00,  3.71it/s]\n",
      "    Loss: 0.3672\n",
      "    Epoch 3/5: 100%|██████████████████████████| 569/569 [02:34<00:00,  3.69it/s]\n",
      "    Loss: 0.2224\n",
      "    Epoch 4/5: 100%|██████████████████████████| 569/569 [02:33<00:00,  3.71it/s]\n",
      "    Loss: 0.1709\n",
      "    Epoch 5/5: 100%|██████████████████████████| 569/569 [02:33<00:00,  3.71it/s]\n",
      "    Loss: 0.1357\n",
      "Training Fold 3/5...\n",
      "Some weights of ExtractionModel were not initialized from the model checkpoint at /mnt/d/HuggingFaceModels/models--hfl--chinese-roberta-wwm-ext/ and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  Training Extraction Model (Fold: fold_2)\n",
      "    Epoch 1/5: 100%|██████████████████████████| 162/162 [00:44<00:00,  3.63it/s]\n",
      "    Loss: 0.3110\n",
      "    Epoch 2/5: 100%|██████████████████████████| 162/162 [00:44<00:00,  3.65it/s]\n",
      "    Loss: 0.1360\n",
      "    Epoch 3/5: 100%|██████████████████████████| 162/162 [00:44<00:00,  3.65it/s]\n",
      "    Loss: 0.0996\n",
      "    Epoch 4/5: 100%|██████████████████████████| 162/162 [00:44<00:00,  3.65it/s]\n",
      "    Loss: 0.0762\n",
      "    Epoch 5/5: 100%|██████████████████████████| 162/162 [00:44<00:00,  3.65it/s]\n",
      "    Loss: 0.0637\n",
      "Some weights of ClassificationModel were not initialized from the model checkpoint at /mnt/d/HuggingFaceModels/models--hfl--chinese-roberta-wwm-ext/ and are newly initialized: ['cat_classifier.bias', 'cat_classifier.weight', 'pol_classifier.bias', 'pol_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  Training Classification Model (Fold: fold_2)\n",
      "    Epoch 1/5: 100%|██████████████████████████| 569/569 [02:33<00:00,  3.71it/s]\n",
      "    Loss: 1.1282\n",
      "    Epoch 2/5: 100%|██████████████████████████| 569/569 [02:33<00:00,  3.71it/s]\n",
      "    Loss: 0.3648\n",
      "    Epoch 3/5: 100%|██████████████████████████| 569/569 [02:33<00:00,  3.71it/s]\n",
      "    Loss: 0.2271\n",
      "    Epoch 4/5: 100%|██████████████████████████| 569/569 [02:33<00:00,  3.71it/s]\n",
      "    Loss: 0.1585\n",
      "    Epoch 5/5: 100%|██████████████████████████| 569/569 [02:33<00:00,  3.71it/s]\n",
      "    Loss: 0.1278\n",
      "Training Fold 4/5...\n",
      "Some weights of ExtractionModel were not initialized from the model checkpoint at /mnt/d/HuggingFaceModels/models--hfl--chinese-roberta-wwm-ext/ and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  Training Extraction Model (Fold: fold_3)\n",
      "    Epoch 1/5: 100%|██████████████████████████| 162/162 [00:44<00:00,  3.63it/s]\n",
      "    Loss: 0.3121\n",
      "    Epoch 2/5: 100%|██████████████████████████| 162/162 [00:44<00:00,  3.64it/s]\n",
      "    Loss: 0.1305\n",
      "    Epoch 3/5: 100%|██████████████████████████| 162/162 [00:44<00:00,  3.64it/s]\n",
      "    Loss: 0.0948\n",
      "    Epoch 4/5: 100%|██████████████████████████| 162/162 [00:44<00:00,  3.65it/s]\n",
      "    Loss: 0.0691\n",
      "    Epoch 5/5: 100%|██████████████████████████| 162/162 [00:44<00:00,  3.65it/s]\n",
      "    Loss: 0.0553\n",
      "Some weights of ClassificationModel were not initialized from the model checkpoint at /mnt/d/HuggingFaceModels/models--hfl--chinese-roberta-wwm-ext/ and are newly initialized: ['cat_classifier.bias', 'cat_classifier.weight', 'pol_classifier.bias', 'pol_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  Training Classification Model (Fold: fold_3)\n",
      "    Epoch 1/5: 100%|██████████████████████████| 570/570 [02:33<00:00,  3.70it/s]\n",
      "    Loss: 1.1223\n",
      "    Epoch 2/5: 100%|██████████████████████████| 570/570 [02:33<00:00,  3.71it/s]\n",
      "    Loss: 0.3595\n",
      "    Epoch 3/5: 100%|██████████████████████████| 570/570 [02:33<00:00,  3.71it/s]\n",
      "    Loss: 0.2184\n",
      "    Epoch 4/5: 100%|██████████████████████████| 570/570 [02:33<00:00,  3.71it/s]\n",
      "    Loss: 0.1611\n",
      "    Epoch 5/5: 100%|██████████████████████████| 570/570 [02:33<00:00,  3.71it/s]\n",
      "    Loss: 0.1233\n",
      "Training Fold 5/5...\n",
      "Some weights of ExtractionModel were not initialized from the model checkpoint at /mnt/d/HuggingFaceModels/models--hfl--chinese-roberta-wwm-ext/ and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  Training Extraction Model (Fold: fold_4)\n",
      "    Epoch 1/5: 100%|██████████████████████████| 162/162 [00:44<00:00,  3.63it/s]\n",
      "    Loss: 0.3237\n",
      "    Epoch 2/5: 100%|██████████████████████████| 162/162 [00:44<00:00,  3.64it/s]\n",
      "    Loss: 0.1365\n",
      "    Epoch 3/5: 100%|██████████████████████████| 162/162 [00:44<00:00,  3.64it/s]\n",
      "    Loss: 0.1004\n",
      "    Epoch 4/5: 100%|██████████████████████████| 162/162 [00:44<00:00,  3.65it/s]\n",
      "    Loss: 0.0761\n",
      "    Epoch 5/5: 100%|██████████████████████████| 162/162 [00:44<00:00,  3.65it/s]\n",
      "    Loss: 0.0617\n",
      "Some weights of ClassificationModel were not initialized from the model checkpoint at /mnt/d/HuggingFaceModels/models--hfl--chinese-roberta-wwm-ext/ and are newly initialized: ['cat_classifier.bias', 'cat_classifier.weight', 'pol_classifier.bias', 'pol_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  Training Classification Model (Fold: fold_4)\n",
      "    Epoch 1/5: 100%|██████████████████████████| 573/573 [02:34<00:00,  3.71it/s]\n",
      "    Loss: 1.1329\n",
      "    Epoch 2/5: 100%|██████████████████████████| 573/573 [02:34<00:00,  3.71it/s]\n",
      "    Loss: 0.3562\n",
      "    Epoch 3/5: 100%|██████████████████████████| 573/573 [02:34<00:00,  3.71it/s]\n",
      "    Loss: 0.2291\n",
      "    Epoch 4/5: 100%|██████████████████████████| 573/573 [02:34<00:00,  3.71it/s]\n",
      "    Loss: 0.1715\n",
      "    Epoch 5/5: 100%|██████████████████████████| 573/573 [02:34<00:00,  3.71it/s]\n",
      "    Loss: 0.1313\n",
      "/home/xiuminke/miniconda3/envs/ml12/lib/python3.11/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "Some weights of ExtractionModel were not initialized from the model checkpoint at /mnt/d/HuggingFaceModels/models--hfl--chinese-roberta-wwm-ext/ and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of ClassificationModel were not initialized from the model checkpoint at /mnt/d/HuggingFaceModels/models--hfl--chinese-roberta-wwm-ext/ and are newly initialized: ['cat_classifier.bias', 'cat_classifier.weight', 'pol_classifier.bias', 'pol_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Fold extraction (fold_0): 100%|█████████████████| 41/41 [00:03<00:00, 10.78it/s]\n",
      "Fold classification (fold_0): 100%|███████████| 144/144 [00:12<00:00, 11.26it/s]\n",
      "------------------------------\n",
      "Fold 0\n",
      "Correct (S): 1025\n",
      "Predicted (P): 1315\n",
      "Ground Truth (G): 1314\n",
      "Precision: 0.7795\n",
      "Recall:    0.7801\n",
      "F1 Score:  0.7798\n",
      "Some weights of ExtractionModel were not initialized from the model checkpoint at /mnt/d/HuggingFaceModels/models--hfl--chinese-roberta-wwm-ext/ and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of ClassificationModel were not initialized from the model checkpoint at /mnt/d/HuggingFaceModels/models--hfl--chinese-roberta-wwm-ext/ and are newly initialized: ['cat_classifier.bias', 'cat_classifier.weight', 'pol_classifier.bias', 'pol_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Fold extraction (fold_1): 100%|█████████████████| 41/41 [00:03<00:00, 10.37it/s]\n",
      "Fold classification (fold_1): 100%|███████████| 142/142 [00:12<00:00, 11.31it/s]\n",
      "------------------------------\n",
      "Fold 1\n",
      "Correct (S): 1020\n",
      "Predicted (P): 1313\n",
      "Ground Truth (G): 1318\n",
      "Precision: 0.7768\n",
      "Recall:    0.7739\n",
      "F1 Score:  0.7754\n",
      "Some weights of ExtractionModel were not initialized from the model checkpoint at /mnt/d/HuggingFaceModels/models--hfl--chinese-roberta-wwm-ext/ and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of ClassificationModel were not initialized from the model checkpoint at /mnt/d/HuggingFaceModels/models--hfl--chinese-roberta-wwm-ext/ and are newly initialized: ['cat_classifier.bias', 'cat_classifier.weight', 'pol_classifier.bias', 'pol_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Fold extraction (fold_2): 100%|█████████████████| 41/41 [00:04<00:00, 10.13it/s]\n",
      "Fold classification (fold_2): 100%|███████████| 152/152 [00:13<00:00, 11.27it/s]\n",
      "------------------------------\n",
      "Fold 2\n",
      "Correct (S): 1036\n",
      "Predicted (P): 1341\n",
      "Ground Truth (G): 1325\n",
      "Precision: 0.7726\n",
      "Recall:    0.7819\n",
      "F1 Score:  0.7772\n",
      "Some weights of ExtractionModel were not initialized from the model checkpoint at /mnt/d/HuggingFaceModels/models--hfl--chinese-roberta-wwm-ext/ and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of ClassificationModel were not initialized from the model checkpoint at /mnt/d/HuggingFaceModels/models--hfl--chinese-roberta-wwm-ext/ and are newly initialized: ['cat_classifier.bias', 'cat_classifier.weight', 'pol_classifier.bias', 'pol_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Fold extraction (fold_3): 100%|█████████████████| 41/41 [00:03<00:00, 10.48it/s]\n",
      "Fold classification (fold_3): 100%|███████████| 146/146 [00:12<00:00, 11.29it/s]\n",
      "------------------------------\n",
      "Fold 3\n",
      "Correct (S): 1064\n",
      "Predicted (P): 1350\n",
      "Ground Truth (G): 1348\n",
      "Precision: 0.7881\n",
      "Recall:    0.7893\n",
      "F1 Score:  0.7887\n",
      "Some weights of ExtractionModel were not initialized from the model checkpoint at /mnt/d/HuggingFaceModels/models--hfl--chinese-roberta-wwm-ext/ and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of ClassificationModel were not initialized from the model checkpoint at /mnt/d/HuggingFaceModels/models--hfl--chinese-roberta-wwm-ext/ and are newly initialized: ['cat_classifier.bias', 'cat_classifier.weight', 'pol_classifier.bias', 'pol_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Fold extraction (fold_4): 100%|█████████████████| 41/41 [00:03<00:00, 10.67it/s]\n",
      "Fold classification (fold_4): 100%|███████████| 143/143 [00:12<00:00, 11.31it/s]\n",
      "------------------------------\n",
      "Fold 4\n",
      "Correct (S): 1041\n",
      "Predicted (P): 1330\n",
      "Ground Truth (G): 1327\n",
      "Precision: 0.7827\n",
      "Recall:    0.7845\n",
      "F1 Score:  0.7836\n",
      "==============================\n",
      "CV Overall\n",
      "Correct (S): 5186\n",
      "Predicted (P): 6649\n",
      "Ground Truth (G): 6632\n",
      "==============================\n",
      "Precision: 0.7800\n",
      "Recall:    0.7820\n",
      "F1 Score:  0.7810\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "!python train_cv.py --num_folds 5 --seed 42 --epochs_extract 5 --epochs_classify 5\n",
    "!python evaluate_f1_cv.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179ad940-1313-4ba1-ba9b-069a454535fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV Overall\n",
    "# Correct (S): 5186\n",
    "# Predicted (P): 6649\n",
    "# Ground Truth (G): 6632\n",
    "# ==============================\n",
    "# Precision: 0.7800\n",
    "# Recall:    0.7820\n",
    "# F1 Score:  0.7810"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990d28d8-b7b7-43db-a8c9-7d43bf26c23d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b95566-c204-4d5c-abf0-ce05e273e7b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7c57987-bb73-4875-bcb7-ddc6ff24f308",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training...\n",
      "/home/xiuminke/miniconda3/envs/ml12/lib/python3.11/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "Using device: cuda\n",
      "Training Extraction Model...\n",
      "Some weights of ExtractionModel were not initialized from the model checkpoint at /mnt/d/HuggingFaceModels/models--hfl--chinese-roberta-wwm-ext/ and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1: 100%|████████████████████████████████| 202/202 [00:55<00:00,  3.61it/s]\n",
      "Loss: 0.2834129164567088\n",
      "Epoch 2: 100%|████████████████████████████████| 202/202 [00:55<00:00,  3.66it/s]\n",
      "Loss: 0.12632904170394535\n",
      "Epoch 3: 100%|████████████████████████████████| 202/202 [00:55<00:00,  3.66it/s]\n",
      "Loss: 0.09095727634791395\n",
      "Epoch 4: 100%|████████████████████████████████| 202/202 [00:55<00:00,  3.65it/s]\n",
      "Loss: 0.06644120319552793\n",
      "Epoch 5: 100%|████████████████████████████████| 202/202 [00:55<00:00,  3.64it/s]\n",
      "Loss: 0.04904076223501401\n",
      "Epoch 6: 100%|████████████████████████████████| 202/202 [00:55<00:00,  3.65it/s]\n",
      "Loss: 0.036630425980259285\n",
      "Epoch 7: 100%|████████████████████████████████| 202/202 [00:55<00:00,  3.65it/s]\n",
      "Loss: 0.02552286274546739\n",
      "Epoch 8: 100%|████████████████████████████████| 202/202 [00:55<00:00,  3.64it/s]\n",
      "Loss: 0.01899726946935021\n",
      "Epoch 9: 100%|████████████████████████████████| 202/202 [00:55<00:00,  3.65it/s]\n",
      "Loss: 0.014578106703345046\n",
      "Epoch 10: 100%|███████████████████████████████| 202/202 [00:55<00:00,  3.64it/s]\n",
      "Loss: 0.011497384297435569\n",
      "Epoch 11: 100%|███████████████████████████████| 202/202 [00:55<00:00,  3.64it/s]\n",
      "Loss: 0.011395068337511511\n",
      "Epoch 12: 100%|███████████████████████████████| 202/202 [00:55<00:00,  3.64it/s]\n",
      "Loss: 0.008224646465179723\n",
      "Epoch 13: 100%|███████████████████████████████| 202/202 [00:55<00:00,  3.65it/s]\n",
      "Loss: 0.007824334244204612\n",
      "Epoch 14: 100%|███████████████████████████████| 202/202 [00:55<00:00,  3.65it/s]\n",
      "Loss: 0.007897149593803322\n",
      "Epoch 15: 100%|███████████████████████████████| 202/202 [00:55<00:00,  3.64it/s]\n",
      "Loss: 0.007139629942721153\n",
      "Epoch 16: 100%|███████████████████████████████| 202/202 [00:55<00:00,  3.64it/s]\n",
      "Loss: 0.005297428621428715\n",
      "Epoch 17: 100%|███████████████████████████████| 202/202 [00:55<00:00,  3.63it/s]\n",
      "Loss: 0.004527572571965036\n",
      "Epoch 18: 100%|███████████████████████████████| 202/202 [00:55<00:00,  3.65it/s]\n",
      "Loss: 0.004483151213423866\n",
      "Epoch 19: 100%|███████████████████████████████| 202/202 [00:55<00:00,  3.64it/s]\n",
      "Loss: 0.003404255502852657\n",
      "Epoch 20: 100%|███████████████████████████████| 202/202 [00:55<00:00,  3.65it/s]\n",
      "Loss: 0.0030456975606410757\n",
      "Epoch 21: 100%|███████████████████████████████| 202/202 [00:55<00:00,  3.64it/s]\n",
      "Loss: 0.002599654861582941\n",
      "Epoch 22: 100%|███████████████████████████████| 202/202 [00:55<00:00,  3.64it/s]\n",
      "Loss: 0.0025273287321739405\n",
      "Epoch 23: 100%|███████████████████████████████| 202/202 [00:55<00:00,  3.64it/s]\n",
      "Loss: 0.0028458923298236224\n",
      "Epoch 24: 100%|███████████████████████████████| 202/202 [00:55<00:00,  3.64it/s]\n",
      "Loss: 0.0023507365432025007\n",
      "Epoch 25: 100%|███████████████████████████████| 202/202 [00:55<00:00,  3.64it/s]\n",
      "Loss: 0.0020015954421366343\n",
      "Training Classification Model...\n",
      "Some weights of ClassificationModel were not initialized from the model checkpoint at /mnt/d/HuggingFaceModels/models--hfl--chinese-roberta-wwm-ext/ and are newly initialized: ['cat_classifier.bias', 'cat_classifier.weight', 'pol_classifier.bias', 'pol_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1: 100%|████████████████████████████████| 713/713 [03:12<00:00,  3.71it/s]\n",
      "Loss: 1.0099318939957345\n",
      "Epoch 2: 100%|████████████████████████████████| 713/713 [03:12<00:00,  3.71it/s]\n",
      "Loss: 0.33441515116420734\n",
      "Epoch 3: 100%|████████████████████████████████| 713/713 [03:12<00:00,  3.71it/s]\n",
      "Loss: 0.23427784077295455\n",
      "Epoch 4: 100%|████████████████████████████████| 713/713 [03:12<00:00,  3.71it/s]\n",
      "Loss: 0.18721998398692502\n",
      "Epoch 5: 100%|████████████████████████████████| 713/713 [03:12<00:00,  3.71it/s]\n",
      "Loss: 0.13471039407657978\n",
      "Epoch 6: 100%|████████████████████████████████| 713/713 [03:12<00:00,  3.71it/s]\n",
      "Loss: 0.1294967985344949\n",
      "Epoch 7: 100%|████████████████████████████████| 713/713 [03:12<00:00,  3.71it/s]\n",
      "Loss: 0.11000127765006512\n",
      "Epoch 8: 100%|████████████████████████████████| 713/713 [03:12<00:00,  3.71it/s]\n",
      "Loss: 0.08120451074747988\n",
      "Epoch 9: 100%|████████████████████████████████| 713/713 [03:12<00:00,  3.71it/s]\n",
      "Loss: 0.06977427281572537\n",
      "Epoch 10: 100%|███████████████████████████████| 713/713 [03:11<00:00,  3.71it/s]\n",
      "Loss: 0.05671379370954499\n",
      "Epoch 11: 100%|███████████████████████████████| 713/713 [03:12<00:00,  3.71it/s]\n",
      "Loss: 0.04887398066855366\n",
      "Epoch 12: 100%|███████████████████████████████| 713/713 [03:11<00:00,  3.72it/s]\n",
      "Loss: 0.0417396515274099\n",
      "Epoch 13: 100%|███████████████████████████████| 713/713 [03:12<00:00,  3.71it/s]\n",
      "Loss: 0.029771529171069847\n",
      "Epoch 14: 100%|███████████████████████████████| 713/713 [03:12<00:00,  3.71it/s]\n",
      "Loss: 0.028909169082780635\n",
      "Epoch 15: 100%|███████████████████████████████| 713/713 [03:12<00:00,  3.71it/s]\n",
      "Loss: 0.019032938021194677\n",
      "Epoch 16: 100%|███████████████████████████████| 713/713 [03:12<00:00,  3.71it/s]\n",
      "Loss: 0.01563383914108374\n",
      "Epoch 17: 100%|███████████████████████████████| 713/713 [03:12<00:00,  3.71it/s]\n",
      "Loss: 0.009250038537979883\n",
      "Epoch 18: 100%|███████████████████████████████| 713/713 [03:11<00:00,  3.72it/s]\n",
      "Loss: 0.0075224034657517674\n",
      "Epoch 19: 100%|███████████████████████████████| 713/713 [03:12<00:00,  3.71it/s]\n",
      "Loss: 0.007310804815564192\n",
      "Epoch 20: 100%|███████████████████████████████| 713/713 [03:13<00:00,  3.69it/s]\n",
      "Loss: 0.004485675991327681\n",
      "Epoch 21: 100%|███████████████████████████████| 713/713 [03:11<00:00,  3.71it/s]\n",
      "Loss: 0.008552637111830236\n",
      "Epoch 22: 100%|███████████████████████████████| 713/713 [03:11<00:00,  3.71it/s]\n",
      "Loss: 0.004760373846809334\n",
      "Epoch 23: 100%|███████████████████████████████| 713/713 [03:12<00:00,  3.71it/s]\n",
      "Loss: 0.002580787239266921\n",
      "Epoch 24: 100%|███████████████████████████████| 713/713 [03:12<00:00,  3.71it/s]\n",
      "Loss: 0.00269492795614318\n",
      "Epoch 25: 100%|███████████████████████████████| 713/713 [03:11<00:00,  3.71it/s]\n",
      "Loss: 0.002485782715472684\n",
      "\n",
      "\n",
      "\n",
      "Starting Prediction...\n",
      "/home/xiuminke/miniconda3/envs/ml12/lib/python3.11/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "Running Extraction...\n",
      "Some weights of ExtractionModel were not initialized from the model checkpoint at /mnt/d/HuggingFaceModels/models--hfl--chinese-roberta-wwm-ext/ and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|█████████████████████████████████████████| 140/140 [00:13<00:00, 10.58it/s]\n",
      "Running Classification...\n",
      "Some weights of ClassificationModel were not initialized from the model checkpoint at /mnt/d/HuggingFaceModels/models--hfl--chinese-roberta-wwm-ext/ and are newly initialized: ['cat_classifier.bias', 'cat_classifier.weight', 'pol_classifier.bias', 'pol_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|█████████████████████████████████████████| 495/495 [00:44<00:00, 11.03it/s]\n",
      "Saved results to /mnt/d/forCoding_data/Tianchi_Ecommerce6/plan_2-あ-基于1-换roberta1/preprocessedData/Result.csv\n",
      "\n",
      "\n",
      "\n",
      "Starting Evaluation...\n",
      "/home/xiuminke/miniconda3/envs/ml12/lib/python3.11/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "Using device: cuda\n",
      "Loading Train Data for Evaluation...\n",
      "Running Extraction on Train Set...\n",
      "Some weights of ExtractionModel were not initialized from the model checkpoint at /mnt/d/HuggingFaceModels/models--hfl--chinese-roberta-wwm-ext/ and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Extraction: 100%|█████████████████████████████| 202/202 [00:19<00:00, 10.45it/s]\n",
      "Running Classification on Candidate Pairs...\n",
      "Some weights of ClassificationModel were not initialized from the model checkpoint at /mnt/d/HuggingFaceModels/models--hfl--chinese-roberta-wwm-ext/ and are newly initialized: ['cat_classifier.bias', 'cat_classifier.weight', 'pol_classifier.bias', 'pol_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Classification: 100%|█████████████████████████| 687/687 [01:02<00:00, 11.05it/s]\n",
      "Computing F1 Score...\n",
      "------------------------------\n",
      "Correct (S): 6452\n",
      "Predicted (P): 6458\n",
      "Ground Truth (G): 6632\n",
      "------------------------------\n",
      "Precision: 0.9991\n",
      "Recall:    0.9729\n",
      "F1 Score:  0.9858\n",
      "------------------------------\n",
      "Error analysis saved to error_analysis.txt\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "!sh run.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b7a92d-9fc7-4e6d-9b1c-017975b9aa04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27144510-1c91-4e2c-bd1f-23e20bc906fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00467717-a2ae-4eea-8fa0-409ecf349f81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d780d2-607d-41d2-992c-b5c905608aaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "97dfdf37-5f20-41a9-83f6-3d9a7ec4db4e",
   "metadata": {},
   "source": [
    "【TODO】~~增大轮数~~，难样本挖掘，数据增强，使用新的数据（可能借助大模型），~~换模型~~。\n",
    "\n",
    "【TODO】polarity中性的样本，换一种处理方式？\n",
    "\n",
    "【TODO】plan_3里面说的EMA、Multi-Sample Dropout可以试试。这两个不是数据增强的手段。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28b1998-8a81-43b2-948c-e7cf77ee3364",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb6abed-b794-421e-a364-d8b416b328d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddf5334-2948-48ee-bca4-a972816f2e88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791b8160-515f-4826-b989-5440ed875c60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
