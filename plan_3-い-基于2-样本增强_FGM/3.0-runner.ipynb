{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e41bae6b-8702-4d6f-9c8c-2e506dfc6886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "storage dir: /mnt/d/forCoding_data/Tianchi_Ecommerce6/plan_3-い-基于2-样本增强_FGM\n",
      "code dir: /mnt/d/forCoding_code/Tianchi_Ecommerce6/plan_3-い-基于2-样本增强_FGM\n"
     ]
    }
   ],
   "source": [
    "with open(\"/mnt/d/forCoding_code/_init_tools/x__params.py\", \"r\") as f:\n",
    "    txt = f.read()\n",
    "exec(txt)\n",
    "\n",
    "with open(\"/mnt/d/forCoding_code/_init_tools/report_macro_0516.py\", \"r\") as f:\n",
    "    txt = f.read()\n",
    "exec(txt)\n",
    "\n",
    "with open(\"/mnt/d/forCoding_code/_init_tools/x__tool_here1.py\", \"r\") as f:\n",
    "    exec(f.read())\n",
    "with open(\"/mnt/d/forCoding_code/_init_tools/x__tool_here2.py\", \"r\") as f:\n",
    "    exec(f.read())\n",
    "\n",
    "wasted_dir = \"./wasted\"\n",
    "if not os.path.exists(wasted_dir):\n",
    "    os.makedirs(wasted_dir)\n",
    "\n",
    "pd.set_option('display.max_rows',200)\n",
    "pd.set_option('display.max_columns',200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df61c3f-0e69-4640-87cd-196d7285a1ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e341566f-88a3-4812-8928-a380b74ce6ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/xiuminke/miniconda3/envs/ml12/lib/python3.11/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "Starting 5-fold CV training.\n",
      "Seed: 42\n",
      "Epochs (Extract): 5\n",
      "Epochs (Classify): 5\n",
      "Output Directory: /mnt/d/forCoding_data/Tianchi_Ecommerce6/plan_3-い-基于2-样本增强_FGM/trained_models/cv\n",
      "Training Fold 1/5...\n",
      "Some weights of ExtractionModel were not initialized from the model checkpoint at /mnt/d/HuggingFaceModels/models--hfl--chinese-roberta-wwm-ext/ and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  Training Extraction Model (Fold: fold_0)\n",
      "    Epoch 1/5: 100%|██████████████████████████| 162/162 [01:24<00:00,  1.91it/s]\n",
      "    Loss: 0.3121\n",
      "    Epoch 2/5: 100%|██████████████████████████| 162/162 [01:25<00:00,  1.90it/s]\n",
      "    Loss: 0.1340\n",
      "    Epoch 3/5: 100%|██████████████████████████| 162/162 [01:25<00:00,  1.89it/s]\n",
      "    Loss: 0.1016\n",
      "    Epoch 4/5: 100%|██████████████████████████| 162/162 [01:25<00:00,  1.89it/s]\n",
      "    Loss: 0.0785\n",
      "    Epoch 5/5: 100%|██████████████████████████| 162/162 [01:25<00:00,  1.90it/s]\n",
      "    Loss: 0.0653\n",
      "Some weights of ClassificationModel were not initialized from the model checkpoint at /mnt/d/HuggingFaceModels/models--hfl--chinese-roberta-wwm-ext/ and are newly initialized: ['cat_classifier.bias', 'cat_classifier.weight', 'pol_classifier.bias', 'pol_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  Training Classification Model (Fold: fold_0)\n",
      "    Epoch 1/5: 100%|██████████████████████████| 569/569 [04:57<00:00,  1.91it/s]\n",
      "    Loss: 1.1192\n",
      "    Epoch 2/5: 100%|██████████████████████████| 569/569 [04:57<00:00,  1.91it/s]\n",
      "    Loss: 0.3348\n",
      "    Epoch 3/5: 100%|██████████████████████████| 569/569 [04:57<00:00,  1.91it/s]\n",
      "    Loss: 0.2107\n",
      "    Epoch 4/5: 100%|██████████████████████████| 569/569 [04:57<00:00,  1.91it/s]\n",
      "    Loss: 0.1540\n",
      "    Epoch 5/5: 100%|██████████████████████████| 569/569 [04:58<00:00,  1.91it/s]\n",
      "    Loss: 0.1275\n",
      "Training Fold 2/5...\n",
      "Some weights of ExtractionModel were not initialized from the model checkpoint at /mnt/d/HuggingFaceModels/models--hfl--chinese-roberta-wwm-ext/ and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  Training Extraction Model (Fold: fold_1)\n",
      "    Epoch 1/5: 100%|██████████████████████████| 162/162 [01:25<00:00,  1.89it/s]\n",
      "    Loss: 0.3116\n",
      "    Epoch 2/5: 100%|██████████████████████████| 162/162 [01:25<00:00,  1.89it/s]\n",
      "    Loss: 0.1338\n",
      "    Epoch 3/5: 100%|██████████████████████████| 162/162 [01:25<00:00,  1.89it/s]\n",
      "    Loss: 0.1015\n",
      "    Epoch 4/5: 100%|██████████████████████████| 162/162 [01:25<00:00,  1.90it/s]\n",
      "    Loss: 0.0811\n",
      "    Epoch 5/5: 100%|██████████████████████████| 162/162 [01:25<00:00,  1.89it/s]\n",
      "    Loss: 0.0676\n",
      "Some weights of ClassificationModel were not initialized from the model checkpoint at /mnt/d/HuggingFaceModels/models--hfl--chinese-roberta-wwm-ext/ and are newly initialized: ['cat_classifier.bias', 'cat_classifier.weight', 'pol_classifier.bias', 'pol_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  Training Classification Model (Fold: fold_1)\n",
      "    Epoch 1/5: 100%|██████████████████████████| 569/569 [04:59<00:00,  1.90it/s]\n",
      "    Loss: 1.1508\n",
      "    Epoch 2/5: 100%|██████████████████████████| 569/569 [04:57<00:00,  1.91it/s]\n",
      "    Loss: 0.3516\n",
      "    Epoch 3/5: 100%|██████████████████████████| 569/569 [04:57<00:00,  1.91it/s]\n",
      "    Loss: 0.2227\n",
      "    Epoch 4/5: 100%|██████████████████████████| 569/569 [04:57<00:00,  1.91it/s]\n",
      "    Loss: 0.1696\n",
      "    Epoch 5/5: 100%|██████████████████████████| 569/569 [04:57<00:00,  1.91it/s]\n",
      "    Loss: 0.1396\n",
      "Training Fold 3/5...\n",
      "Some weights of ExtractionModel were not initialized from the model checkpoint at /mnt/d/HuggingFaceModels/models--hfl--chinese-roberta-wwm-ext/ and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  Training Extraction Model (Fold: fold_2)\n",
      "    Epoch 1/5: 100%|██████████████████████████| 162/162 [01:25<00:00,  1.89it/s]\n",
      "    Loss: 0.3083\n",
      "    Epoch 2/5: 100%|██████████████████████████| 162/162 [01:25<00:00,  1.89it/s]\n",
      "    Loss: 0.1391\n",
      "    Epoch 3/5: 100%|██████████████████████████| 162/162 [01:25<00:00,  1.89it/s]\n",
      "    Loss: 0.1042\n",
      "    Epoch 4/5: 100%|██████████████████████████| 162/162 [01:25<00:00,  1.89it/s]\n",
      "    Loss: 0.0830\n",
      "    Epoch 5/5: 100%|██████████████████████████| 162/162 [01:25<00:00,  1.89it/s]\n",
      "    Loss: 0.0696\n",
      "Some weights of ClassificationModel were not initialized from the model checkpoint at /mnt/d/HuggingFaceModels/models--hfl--chinese-roberta-wwm-ext/ and are newly initialized: ['cat_classifier.bias', 'cat_classifier.weight', 'pol_classifier.bias', 'pol_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  Training Classification Model (Fold: fold_2)\n",
      "    Epoch 1/5: 100%|██████████████████████████| 569/569 [04:58<00:00,  1.91it/s]\n",
      "    Loss: 1.1200\n",
      "    Epoch 2/5: 100%|██████████████████████████| 569/569 [04:57<00:00,  1.91it/s]\n",
      "    Loss: 0.3487\n",
      "    Epoch 3/5: 100%|██████████████████████████| 569/569 [04:57<00:00,  1.91it/s]\n",
      "    Loss: 0.2260\n",
      "    Epoch 4/5: 100%|██████████████████████████| 569/569 [04:57<00:00,  1.91it/s]\n",
      "    Loss: 0.1664\n",
      "    Epoch 5/5: 100%|██████████████████████████| 569/569 [04:58<00:00,  1.91it/s]\n",
      "    Loss: 0.1360\n",
      "Training Fold 4/5...\n",
      "Some weights of ExtractionModel were not initialized from the model checkpoint at /mnt/d/HuggingFaceModels/models--hfl--chinese-roberta-wwm-ext/ and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  Training Extraction Model (Fold: fold_3)\n",
      "    Epoch 1/5: 100%|██████████████████████████| 162/162 [01:25<00:00,  1.89it/s]\n",
      "    Loss: 0.3096\n",
      "    Epoch 2/5: 100%|██████████████████████████| 162/162 [01:25<00:00,  1.89it/s]\n",
      "    Loss: 0.1335\n",
      "    Epoch 3/5: 100%|██████████████████████████| 162/162 [01:26<00:00,  1.88it/s]\n",
      "    Loss: 0.0977\n",
      "    Epoch 4/5: 100%|██████████████████████████| 162/162 [01:25<00:00,  1.90it/s]\n",
      "    Loss: 0.0760\n",
      "    Epoch 5/5: 100%|██████████████████████████| 162/162 [01:25<00:00,  1.89it/s]\n",
      "    Loss: 0.0637\n",
      "Some weights of ClassificationModel were not initialized from the model checkpoint at /mnt/d/HuggingFaceModels/models--hfl--chinese-roberta-wwm-ext/ and are newly initialized: ['cat_classifier.bias', 'cat_classifier.weight', 'pol_classifier.bias', 'pol_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  Training Classification Model (Fold: fold_3)\n",
      "    Epoch 1/5: 100%|██████████████████████████| 570/570 [04:58<00:00,  1.91it/s]\n",
      "    Loss: 1.1110\n",
      "    Epoch 2/5: 100%|██████████████████████████| 570/570 [04:58<00:00,  1.91it/s]\n",
      "    Loss: 0.3366\n",
      "    Epoch 3/5: 100%|██████████████████████████| 570/570 [04:58<00:00,  1.91it/s]\n",
      "    Loss: 0.2172\n",
      "    Epoch 4/5: 100%|██████████████████████████| 570/570 [04:58<00:00,  1.91it/s]\n",
      "    Loss: 0.1638\n",
      "    Epoch 5/5: 100%|██████████████████████████| 570/570 [04:58<00:00,  1.91it/s]\n",
      "    Loss: 0.1355\n",
      "Training Fold 5/5...\n",
      "Some weights of ExtractionModel were not initialized from the model checkpoint at /mnt/d/HuggingFaceModels/models--hfl--chinese-roberta-wwm-ext/ and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  Training Extraction Model (Fold: fold_4)\n",
      "    Epoch 1/5: 100%|██████████████████████████| 162/162 [01:25<00:00,  1.89it/s]\n",
      "    Loss: 0.3214\n",
      "    Epoch 2/5: 100%|██████████████████████████| 162/162 [01:26<00:00,  1.88it/s]\n",
      "    Loss: 0.1391\n",
      "    Epoch 3/5: 100%|██████████████████████████| 162/162 [01:25<00:00,  1.90it/s]\n",
      "    Loss: 0.1067\n",
      "    Epoch 4/5: 100%|██████████████████████████| 162/162 [01:25<00:00,  1.90it/s]\n",
      "    Loss: 0.0835\n",
      "    Epoch 5/5: 100%|██████████████████████████| 162/162 [01:25<00:00,  1.89it/s]\n",
      "    Loss: 0.0695\n",
      "Some weights of ClassificationModel were not initialized from the model checkpoint at /mnt/d/HuggingFaceModels/models--hfl--chinese-roberta-wwm-ext/ and are newly initialized: ['cat_classifier.bias', 'cat_classifier.weight', 'pol_classifier.bias', 'pol_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  Training Classification Model (Fold: fold_4)\n",
      "    Epoch 1/5: 100%|██████████████████████████| 573/573 [05:00<00:00,  1.91it/s]\n",
      "    Loss: 1.1060\n",
      "    Epoch 2/5: 100%|██████████████████████████| 573/573 [04:59<00:00,  1.91it/s]\n",
      "    Loss: 0.3470\n",
      "    Epoch 3/5: 100%|██████████████████████████| 573/573 [05:00<00:00,  1.91it/s]\n",
      "    Loss: 0.2215\n",
      "    Epoch 4/5: 100%|██████████████████████████| 573/573 [05:00<00:00,  1.91it/s]\n",
      "    Loss: 0.1709\n",
      "    Epoch 5/5: 100%|██████████████████████████| 573/573 [04:59<00:00,  1.91it/s]\n",
      "    Loss: 0.1391\n",
      "/home/xiuminke/miniconda3/envs/ml12/lib/python3.11/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "Some weights of ExtractionModel were not initialized from the model checkpoint at /mnt/d/HuggingFaceModels/models--hfl--chinese-roberta-wwm-ext/ and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of ClassificationModel were not initialized from the model checkpoint at /mnt/d/HuggingFaceModels/models--hfl--chinese-roberta-wwm-ext/ and are newly initialized: ['cat_classifier.bias', 'cat_classifier.weight', 'pol_classifier.bias', 'pol_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Fold extraction (fold_0): 100%|█████████████████| 41/41 [00:03<00:00, 10.53it/s]\n",
      "Fold classification (fold_0): 100%|███████████| 146/146 [00:12<00:00, 11.58it/s]\n",
      "------------------------------\n",
      "Fold 0\n",
      "Correct (S): 1039\n",
      "Predicted (P): 1317\n",
      "Ground Truth (G): 1314\n",
      "Precision: 0.7889\n",
      "Recall:    0.7907\n",
      "F1 Score:  0.7898\n",
      "Some weights of ExtractionModel were not initialized from the model checkpoint at /mnt/d/HuggingFaceModels/models--hfl--chinese-roberta-wwm-ext/ and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of ClassificationModel were not initialized from the model checkpoint at /mnt/d/HuggingFaceModels/models--hfl--chinese-roberta-wwm-ext/ and are newly initialized: ['cat_classifier.bias', 'cat_classifier.weight', 'pol_classifier.bias', 'pol_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Fold extraction (fold_1): 100%|█████████████████| 41/41 [00:04<00:00,  9.99it/s]\n",
      "Fold classification (fold_1): 100%|███████████| 143/143 [00:12<00:00, 11.62it/s]\n",
      "------------------------------\n",
      "Fold 1\n",
      "Correct (S): 1030\n",
      "Predicted (P): 1320\n",
      "Ground Truth (G): 1318\n",
      "Precision: 0.7803\n",
      "Recall:    0.7815\n",
      "F1 Score:  0.7809\n",
      "Some weights of ExtractionModel were not initialized from the model checkpoint at /mnt/d/HuggingFaceModels/models--hfl--chinese-roberta-wwm-ext/ and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of ClassificationModel were not initialized from the model checkpoint at /mnt/d/HuggingFaceModels/models--hfl--chinese-roberta-wwm-ext/ and are newly initialized: ['cat_classifier.bias', 'cat_classifier.weight', 'pol_classifier.bias', 'pol_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Fold extraction (fold_2): 100%|█████████████████| 41/41 [00:04<00:00, 10.09it/s]\n",
      "Fold classification (fold_2): 100%|███████████| 151/151 [00:13<00:00, 11.06it/s]\n",
      "------------------------------\n",
      "Fold 2\n",
      "Correct (S): 1045\n",
      "Predicted (P): 1322\n",
      "Ground Truth (G): 1325\n",
      "Precision: 0.7905\n",
      "Recall:    0.7887\n",
      "F1 Score:  0.7896\n",
      "Some weights of ExtractionModel were not initialized from the model checkpoint at /mnt/d/HuggingFaceModels/models--hfl--chinese-roberta-wwm-ext/ and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of ClassificationModel were not initialized from the model checkpoint at /mnt/d/HuggingFaceModels/models--hfl--chinese-roberta-wwm-ext/ and are newly initialized: ['cat_classifier.bias', 'cat_classifier.weight', 'pol_classifier.bias', 'pol_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Fold extraction (fold_3): 100%|█████████████████| 41/41 [00:03<00:00, 10.28it/s]\n",
      "Fold classification (fold_3): 100%|███████████| 146/146 [00:13<00:00, 11.01it/s]\n",
      "------------------------------\n",
      "Fold 3\n",
      "Correct (S): 1065\n",
      "Predicted (P): 1371\n",
      "Ground Truth (G): 1348\n",
      "Precision: 0.7768\n",
      "Recall:    0.7901\n",
      "F1 Score:  0.7834\n",
      "Some weights of ExtractionModel were not initialized from the model checkpoint at /mnt/d/HuggingFaceModels/models--hfl--chinese-roberta-wwm-ext/ and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of ClassificationModel were not initialized from the model checkpoint at /mnt/d/HuggingFaceModels/models--hfl--chinese-roberta-wwm-ext/ and are newly initialized: ['cat_classifier.bias', 'cat_classifier.weight', 'pol_classifier.bias', 'pol_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Fold extraction (fold_4): 100%|█████████████████| 41/41 [00:04<00:00, 10.22it/s]\n",
      "Fold classification (fold_4): 100%|███████████| 143/143 [00:12<00:00, 11.56it/s]\n",
      "------------------------------\n",
      "Fold 4\n",
      "Correct (S): 1073\n",
      "Predicted (P): 1326\n",
      "Ground Truth (G): 1327\n",
      "Precision: 0.8092\n",
      "Recall:    0.8086\n",
      "F1 Score:  0.8089\n",
      "==============================\n",
      "CV Overall\n",
      "Correct (S): 5252\n",
      "Predicted (P): 6656\n",
      "Ground Truth (G): 6632\n",
      "==============================\n",
      "Precision: 0.7891\n",
      "Recall:    0.7919\n",
      "F1 Score:  0.7905\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "!python train_cv.py --num_folds 5 --seed 42 --epochs_extract 5 --epochs_classify 5\n",
    "!python evaluate_f1_cv.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "179ad940-1313-4ba1-ba9b-069a454535fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV Overall\n",
    "# Correct (S): 5252\n",
    "# Predicted (P): 6656\n",
    "# Ground Truth (G): 6632\n",
    "# ==============================\n",
    "# Precision: 0.7891\n",
    "# Recall:    0.7919\n",
    "# F1 Score:  0.7905\n",
    "# =============================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed84ac0-5b8b-4e4b-ab2f-e01645a22bc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b95566-c204-4d5c-abf0-ce05e273e7b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4723413-94c6-4654-b0d2-415b0e83176e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!sh run.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d780d2-607d-41d2-992c-b5c905608aaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "97dfdf37-5f20-41a9-83f6-3d9a7ec4db4e",
   "metadata": {},
   "source": [
    "【TODO】~~增大轮数~~，难样本挖掘，数据增强，使用新的数据（可能借助大模型），~~换模型~~。\n",
    "\n",
    "【TODO】polarity中性的样本，换一种处理方式？\n",
    "\n",
    "【TODO】plan_3里面说的EMA、Multi-Sample Dropout可以试试。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28b1998-8a81-43b2-948c-e7cf77ee3364",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb6abed-b794-421e-a364-d8b416b328d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddf5334-2948-48ee-bca4-a972816f2e88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791b8160-515f-4826-b989-5440ed875c60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
