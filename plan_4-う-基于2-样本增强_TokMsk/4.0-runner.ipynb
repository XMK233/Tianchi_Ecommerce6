{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e41bae6b-8702-4d6f-9c8c-2e506dfc6886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "storage dir: /mnt/d/forCoding_data/Tianchi_Ecommerce6/plan_4-う-基于2-样本增强_TokMsk\n",
      "code dir: /mnt/d/forCoding_code/Tianchi_Ecommerce6/plan_4-う-基于2-样本增强_TokMsk\n"
     ]
    }
   ],
   "source": [
    "with open(\"/mnt/d/forCoding_code/_init_tools/x__params.py\", \"r\") as f:\n",
    "    txt = f.read()\n",
    "exec(txt)\n",
    "\n",
    "with open(\"/mnt/d/forCoding_code/_init_tools/report_macro_0516.py\", \"r\") as f:\n",
    "    txt = f.read()\n",
    "exec(txt)\n",
    "\n",
    "with open(\"/mnt/d/forCoding_code/_init_tools/x__tool_here1.py\", \"r\") as f:\n",
    "    exec(f.read())\n",
    "with open(\"/mnt/d/forCoding_code/_init_tools/x__tool_here2.py\", \"r\") as f:\n",
    "    exec(f.read())\n",
    "\n",
    "wasted_dir = \"./wasted\"\n",
    "if not os.path.exists(wasted_dir):\n",
    "    os.makedirs(wasted_dir)\n",
    "\n",
    "pd.set_option('display.max_rows',200)\n",
    "pd.set_option('display.max_columns',200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df61c3f-0e69-4640-87cd-196d7285a1ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e341566f-88a3-4812-8928-a380b74ce6ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/xiuminke/miniconda3/envs/ml12/lib/python3.11/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "Starting 5-fold CV training.\n",
      "Seed: 42\n",
      "Epochs (Extract): 5\n",
      "Epochs (Classify): 5\n",
      "Output Directory: /mnt/d/forCoding_data/Tianchi_Ecommerce6/plan_4-う-基于2-样本增强_TokMsk/trained_models/cv\n",
      "Training Fold 1/5...\n",
      "Some weights of ExtractionModel were not initialized from the model checkpoint at /mnt/d/HuggingFaceModels/models--hfl--chinese-roberta-wwm-ext/ and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  Training Extraction Model (Fold: fold_0)\n",
      "    Epoch 1/5: 100%|██████████████████████████| 162/162 [00:45<00:00,  3.59it/s]\n",
      "    Loss: 0.3720\n",
      "    Epoch 2/5: 100%|██████████████████████████| 162/162 [00:43<00:00,  3.69it/s]\n",
      "    Loss: 0.1831\n",
      "    Epoch 3/5: 100%|██████████████████████████| 162/162 [00:45<00:00,  3.58it/s]\n",
      "    Loss: 0.1563\n",
      "    Epoch 4/5: 100%|██████████████████████████| 162/162 [00:43<00:00,  3.70it/s]\n",
      "    Loss: 0.1368\n",
      "    Epoch 5/5: 100%|██████████████████████████| 162/162 [00:45<00:00,  3.58it/s]\n",
      "    Loss: 0.1284\n",
      "Some weights of ClassificationModel were not initialized from the model checkpoint at /mnt/d/HuggingFaceModels/models--hfl--chinese-roberta-wwm-ext/ and are newly initialized: ['cat_classifier.bias', 'cat_classifier.weight', 'pol_classifier.bias', 'pol_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  Training Classification Model (Fold: fold_0)\n",
      "    Epoch 1/5: 100%|██████████████████████████| 569/569 [02:34<00:00,  3.69it/s]\n",
      "    Loss: 1.4768\n",
      "    Epoch 2/5: 100%|██████████████████████████| 569/569 [02:33<00:00,  3.70it/s]\n",
      "    Loss: 0.6319\n",
      "    Epoch 3/5: 100%|██████████████████████████| 569/569 [02:33<00:00,  3.70it/s]\n",
      "    Loss: 0.4692\n",
      "    Epoch 4/5: 100%|██████████████████████████| 569/569 [02:33<00:00,  3.70it/s]\n",
      "    Loss: 0.3825\n",
      "    Epoch 5/5: 100%|██████████████████████████| 569/569 [02:33<00:00,  3.72it/s]\n",
      "    Loss: 0.3102\n",
      "Training Fold 2/5...\n",
      "Some weights of ExtractionModel were not initialized from the model checkpoint at /mnt/d/HuggingFaceModels/models--hfl--chinese-roberta-wwm-ext/ and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  Training Extraction Model (Fold: fold_1)\n",
      "    Epoch 1/5: 100%|██████████████████████████| 162/162 [00:44<00:00,  3.66it/s]\n",
      "    Loss: 0.3857\n",
      "    Epoch 2/5: 100%|██████████████████████████| 162/162 [00:45<00:00,  3.58it/s]\n",
      "    Loss: 0.1856\n",
      "    Epoch 3/5: 100%|██████████████████████████| 162/162 [00:45<00:00,  3.59it/s]\n",
      "    Loss: 0.1598\n",
      "    Epoch 4/5: 100%|██████████████████████████| 162/162 [00:43<00:00,  3.70it/s]\n",
      "    Loss: 0.1411\n",
      "    Epoch 5/5: 100%|██████████████████████████| 162/162 [00:45<00:00,  3.57it/s]\n",
      "    Loss: 0.1276\n",
      "Some weights of ClassificationModel were not initialized from the model checkpoint at /mnt/d/HuggingFaceModels/models--hfl--chinese-roberta-wwm-ext/ and are newly initialized: ['cat_classifier.bias', 'cat_classifier.weight', 'pol_classifier.bias', 'pol_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  Training Classification Model (Fold: fold_1)\n",
      "    Epoch 1/5: 100%|██████████████████████████| 569/569 [02:33<00:00,  3.70it/s]\n",
      "    Loss: 1.5719\n",
      "    Epoch 2/5: 100%|██████████████████████████| 569/569 [02:33<00:00,  3.70it/s]\n",
      "    Loss: 0.6577\n",
      "    Epoch 3/5: 100%|██████████████████████████| 569/569 [02:33<00:00,  3.70it/s]\n",
      "    Loss: 0.4966\n",
      "    Epoch 4/5: 100%|██████████████████████████| 569/569 [02:33<00:00,  3.70it/s]\n",
      "    Loss: 0.3992\n",
      "    Epoch 5/5: 100%|██████████████████████████| 569/569 [02:33<00:00,  3.71it/s]\n",
      "    Loss: 0.3569\n",
      "Training Fold 3/5...\n",
      "Some weights of ExtractionModel were not initialized from the model checkpoint at /mnt/d/HuggingFaceModels/models--hfl--chinese-roberta-wwm-ext/ and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  Training Extraction Model (Fold: fold_2)\n",
      "    Epoch 1/5: 100%|██████████████████████████| 162/162 [00:44<00:00,  3.68it/s]\n",
      "    Loss: 0.3973\n",
      "    Epoch 2/5: 100%|██████████████████████████| 162/162 [00:45<00:00,  3.59it/s]\n",
      "    Loss: 0.2006\n",
      "    Epoch 3/5: 100%|██████████████████████████| 162/162 [00:43<00:00,  3.69it/s]\n",
      "    Loss: 0.1709\n",
      "    Epoch 4/5: 100%|██████████████████████████| 162/162 [00:45<00:00,  3.58it/s]\n",
      "    Loss: 0.1530\n",
      "    Epoch 5/5: 100%|██████████████████████████| 162/162 [00:45<00:00,  3.59it/s]\n",
      "    Loss: 0.1428\n",
      "Some weights of ClassificationModel were not initialized from the model checkpoint at /mnt/d/HuggingFaceModels/models--hfl--chinese-roberta-wwm-ext/ and are newly initialized: ['cat_classifier.bias', 'cat_classifier.weight', 'pol_classifier.bias', 'pol_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  Training Classification Model (Fold: fold_2)\n",
      "    Epoch 1/5: 100%|██████████████████████████| 569/569 [02:34<00:00,  3.69it/s]\n",
      "    Loss: 1.4730\n",
      "    Epoch 2/5: 100%|██████████████████████████| 569/569 [02:34<00:00,  3.68it/s]\n",
      "    Loss: 0.6271\n",
      "    Epoch 3/5: 100%|██████████████████████████| 569/569 [02:33<00:00,  3.70it/s]\n",
      "    Loss: 0.4837\n",
      "    Epoch 4/5: 100%|██████████████████████████| 569/569 [02:34<00:00,  3.68it/s]\n",
      "    Loss: 0.3869\n",
      "    Epoch 5/5: 100%|██████████████████████████| 569/569 [02:33<00:00,  3.71it/s]\n",
      "    Loss: 0.3553\n",
      "Training Fold 4/5...\n",
      "Some weights of ExtractionModel were not initialized from the model checkpoint at /mnt/d/HuggingFaceModels/models--hfl--chinese-roberta-wwm-ext/ and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  Training Extraction Model (Fold: fold_3)\n",
      "    Epoch 1/5: 100%|██████████████████████████| 162/162 [00:43<00:00,  3.69it/s]\n",
      "    Loss: 0.3978\n",
      "    Epoch 2/5: 100%|██████████████████████████| 162/162 [00:45<00:00,  3.58it/s]\n",
      "    Loss: 0.1898\n",
      "    Epoch 3/5: 100%|██████████████████████████| 162/162 [00:45<00:00,  3.59it/s]\n",
      "    Loss: 0.1641\n",
      "    Epoch 4/5: 100%|██████████████████████████| 162/162 [00:43<00:00,  3.70it/s]\n",
      "    Loss: 0.1428\n",
      "    Epoch 5/5: 100%|██████████████████████████| 162/162 [00:45<00:00,  3.58it/s]\n",
      "    Loss: 0.1333\n",
      "Some weights of ClassificationModel were not initialized from the model checkpoint at /mnt/d/HuggingFaceModels/models--hfl--chinese-roberta-wwm-ext/ and are newly initialized: ['cat_classifier.bias', 'cat_classifier.weight', 'pol_classifier.bias', 'pol_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  Training Classification Model (Fold: fold_3)\n",
      "    Epoch 1/5: 100%|██████████████████████████| 570/570 [02:34<00:00,  3.69it/s]\n",
      "    Loss: 1.5172\n",
      "    Epoch 2/5: 100%|██████████████████████████| 570/570 [02:34<00:00,  3.70it/s]\n",
      "    Loss: 0.6300\n",
      "    Epoch 3/5: 100%|██████████████████████████| 570/570 [02:34<00:00,  3.70it/s]\n",
      "    Loss: 0.4845\n",
      "    Epoch 4/5: 100%|██████████████████████████| 570/570 [02:33<00:00,  3.72it/s]\n",
      "    Loss: 0.3907\n",
      "    Epoch 5/5: 100%|██████████████████████████| 570/570 [02:33<00:00,  3.70it/s]\n",
      "    Loss: 0.3417\n",
      "Training Fold 5/5...\n",
      "Some weights of ExtractionModel were not initialized from the model checkpoint at /mnt/d/HuggingFaceModels/models--hfl--chinese-roberta-wwm-ext/ and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  Training Extraction Model (Fold: fold_4)\n",
      "    Epoch 1/5: 100%|██████████████████████████| 162/162 [00:45<00:00,  3.53it/s]\n",
      "    Loss: 0.3843\n",
      "    Epoch 2/5: 100%|██████████████████████████| 162/162 [00:44<00:00,  3.62it/s]\n",
      "    Loss: 0.1916\n",
      "    Epoch 3/5: 100%|██████████████████████████| 162/162 [00:45<00:00,  3.57it/s]\n",
      "    Loss: 0.1655\n",
      "    Epoch 4/5: 100%|██████████████████████████| 162/162 [00:43<00:00,  3.70it/s]\n",
      "    Loss: 0.1447\n",
      "    Epoch 5/5: 100%|██████████████████████████| 162/162 [00:45<00:00,  3.59it/s]\n",
      "    Loss: 0.1356\n",
      "Some weights of ClassificationModel were not initialized from the model checkpoint at /mnt/d/HuggingFaceModels/models--hfl--chinese-roberta-wwm-ext/ and are newly initialized: ['cat_classifier.bias', 'cat_classifier.weight', 'pol_classifier.bias', 'pol_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  Training Classification Model (Fold: fold_4)\n",
      "    Epoch 1/5: 100%|██████████████████████████| 573/573 [02:34<00:00,  3.70it/s]\n",
      "    Loss: 1.6315\n",
      "    Epoch 2/5: 100%|██████████████████████████| 573/573 [02:34<00:00,  3.70it/s]\n",
      "    Loss: 0.6820\n",
      "    Epoch 3/5: 100%|██████████████████████████| 573/573 [02:35<00:00,  3.69it/s]\n",
      "    Loss: 0.4902\n",
      "    Epoch 4/5: 100%|██████████████████████████| 573/573 [02:35<00:00,  3.69it/s]\n",
      "    Loss: 0.3961\n",
      "    Epoch 5/5: 100%|██████████████████████████| 573/573 [02:34<00:00,  3.70it/s]\n",
      "    Loss: 0.3661\n",
      "/home/xiuminke/miniconda3/envs/ml12/lib/python3.11/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "Some weights of ExtractionModel were not initialized from the model checkpoint at /mnt/d/HuggingFaceModels/models--hfl--chinese-roberta-wwm-ext/ and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of ClassificationModel were not initialized from the model checkpoint at /mnt/d/HuggingFaceModels/models--hfl--chinese-roberta-wwm-ext/ and are newly initialized: ['cat_classifier.bias', 'cat_classifier.weight', 'pol_classifier.bias', 'pol_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Fold extraction (fold_0): 100%|█████████████████| 41/41 [00:03<00:00, 10.32it/s]\n",
      "Fold classification (fold_0): 100%|███████████| 145/145 [00:13<00:00, 10.76it/s]\n",
      "------------------------------\n",
      "Fold 0\n",
      "Correct (S): 1029\n",
      "Predicted (P): 1323\n",
      "Ground Truth (G): 1314\n",
      "Precision: 0.7778\n",
      "Recall:    0.7831\n",
      "F1 Score:  0.7804\n",
      "Some weights of ExtractionModel were not initialized from the model checkpoint at /mnt/d/HuggingFaceModels/models--hfl--chinese-roberta-wwm-ext/ and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of ClassificationModel were not initialized from the model checkpoint at /mnt/d/HuggingFaceModels/models--hfl--chinese-roberta-wwm-ext/ and are newly initialized: ['cat_classifier.bias', 'cat_classifier.weight', 'pol_classifier.bias', 'pol_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Fold extraction (fold_1): 100%|█████████████████| 41/41 [00:02<00:00, 15.53it/s]\n",
      "Fold classification (fold_1): 100%|███████████| 140/140 [00:13<00:00, 10.76it/s]\n",
      "------------------------------\n",
      "Fold 1\n",
      "Correct (S): 1020\n",
      "Predicted (P): 1306\n",
      "Ground Truth (G): 1318\n",
      "Precision: 0.7810\n",
      "Recall:    0.7739\n",
      "F1 Score:  0.7774\n",
      "Some weights of ExtractionModel were not initialized from the model checkpoint at /mnt/d/HuggingFaceModels/models--hfl--chinese-roberta-wwm-ext/ and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of ClassificationModel were not initialized from the model checkpoint at /mnt/d/HuggingFaceModels/models--hfl--chinese-roberta-wwm-ext/ and are newly initialized: ['cat_classifier.bias', 'cat_classifier.weight', 'pol_classifier.bias', 'pol_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Fold extraction (fold_2): 100%|█████████████████| 41/41 [00:04<00:00,  9.63it/s]\n",
      "Fold classification (fold_2): 100%|███████████| 149/149 [00:12<00:00, 11.92it/s]\n",
      "------------------------------\n",
      "Fold 2\n",
      "Correct (S): 1026\n",
      "Predicted (P): 1310\n",
      "Ground Truth (G): 1325\n",
      "Precision: 0.7832\n",
      "Recall:    0.7743\n",
      "F1 Score:  0.7787\n",
      "Some weights of ExtractionModel were not initialized from the model checkpoint at /mnt/d/HuggingFaceModels/models--hfl--chinese-roberta-wwm-ext/ and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of ClassificationModel were not initialized from the model checkpoint at /mnt/d/HuggingFaceModels/models--hfl--chinese-roberta-wwm-ext/ and are newly initialized: ['cat_classifier.bias', 'cat_classifier.weight', 'pol_classifier.bias', 'pol_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Fold extraction (fold_3): 100%|█████████████████| 41/41 [00:04<00:00,  9.83it/s]\n",
      "Fold classification (fold_3): 100%|███████████| 142/142 [00:13<00:00, 10.79it/s]\n",
      "------------------------------\n",
      "Fold 3\n",
      "Correct (S): 1036\n",
      "Predicted (P): 1329\n",
      "Ground Truth (G): 1348\n",
      "Precision: 0.7795\n",
      "Recall:    0.7685\n",
      "F1 Score:  0.7740\n",
      "Some weights of ExtractionModel were not initialized from the model checkpoint at /mnt/d/HuggingFaceModels/models--hfl--chinese-roberta-wwm-ext/ and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of ClassificationModel were not initialized from the model checkpoint at /mnt/d/HuggingFaceModels/models--hfl--chinese-roberta-wwm-ext/ and are newly initialized: ['cat_classifier.bias', 'cat_classifier.weight', 'pol_classifier.bias', 'pol_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Fold extraction (fold_4): 100%|█████████████████| 41/41 [00:04<00:00, 10.17it/s]\n",
      "Fold classification (fold_4): 100%|███████████| 145/145 [00:13<00:00, 10.77it/s]\n",
      "------------------------------\n",
      "Fold 4\n",
      "Correct (S): 1069\n",
      "Predicted (P): 1351\n",
      "Ground Truth (G): 1327\n",
      "Precision: 0.7913\n",
      "Recall:    0.8056\n",
      "F1 Score:  0.7984\n",
      "==============================\n",
      "CV Overall\n",
      "Correct (S): 5180\n",
      "Predicted (P): 6619\n",
      "Ground Truth (G): 6632\n",
      "==============================\n",
      "Precision: 0.7826\n",
      "Recall:    0.7811\n",
      "F1 Score:  0.7818\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "!python train_cv.py --num_folds 5 --seed 42 --epochs_extract 5 --epochs_classify 5\n",
    "!python evaluate_f1_cv.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92a2b17-54d2-43e9-b41e-f59ef1cb0e6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179ad940-1313-4ba1-ba9b-069a454535fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990d28d8-b7b7-43db-a8c9-7d43bf26c23d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b95566-c204-4d5c-abf0-ce05e273e7b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c57987-bb73-4875-bcb7-ddc6ff24f308",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!sh run.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b7a92d-9fc7-4e6d-9b1c-017975b9aa04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27144510-1c91-4e2c-bd1f-23e20bc906fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00467717-a2ae-4eea-8fa0-409ecf349f81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d780d2-607d-41d2-992c-b5c905608aaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "97dfdf37-5f20-41a9-83f6-3d9a7ec4db4e",
   "metadata": {},
   "source": [
    "【TODO】~~增大轮数~~，难样本挖掘，数据增强，使用新的数据（可能借助大模型），~~换模型~~。\n",
    "\n",
    "【TODO】polarity中性的样本，换一种处理方式？\n",
    "\n",
    "【TODO】plan_3里面说的EMA、Multi-Sample Dropout可以试试。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28b1998-8a81-43b2-948c-e7cf77ee3364",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb6abed-b794-421e-a364-d8b416b328d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddf5334-2948-48ee-bca4-a972816f2e88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791b8160-515f-4826-b989-5440ed875c60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
