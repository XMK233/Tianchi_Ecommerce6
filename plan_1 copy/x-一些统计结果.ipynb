{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e41bae6b-8702-4d6f-9c8c-2e506dfc6886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "storage dir: /mnt/d/forCoding_data/Tianchi_Ecommerce6/plan_1\n",
      "code dir: /mnt/d/forCoding_code/Tianchi_Ecommerce6/plan_1\n"
     ]
    }
   ],
   "source": [
    "with open(\"/mnt/d/forCoding_code/_init_tools/x__params.py\", \"r\") as f:\n",
    "    txt = f.read()\n",
    "exec(txt)\n",
    "\n",
    "with open(\"/mnt/d/forCoding_code/_init_tools/report_macro_0516.py\", \"r\") as f:\n",
    "    txt = f.read()\n",
    "exec(txt)\n",
    "\n",
    "with open(\"/mnt/d/forCoding_code/_init_tools/x__tool_here1.py\", \"r\") as f:\n",
    "    exec(f.read())\n",
    "with open(\"/mnt/d/forCoding_code/_init_tools/x__tool_here2.py\", \"r\") as f:\n",
    "    exec(f.read())\n",
    "\n",
    "wasted_dir = \"./wasted\"\n",
    "if not os.path.exists(wasted_dir):\n",
    "    os.makedirs(wasted_dir)\n",
    "\n",
    "pd.set_option('display.max_rows',200)\n",
    "pd.set_option('display.max_columns',200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3c6d81-1e9a-46fa-b900-bf69acb2717e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0c01137-858f-4146-a5f1-f832ddbd65d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总样本数: 11393\n",
      "将展示样本索引 [0, 4]\n",
      "================================================================================\n",
      "样本索引 idx = 0\n",
      "---- 原始 classification 样本（prepare_classification_samples 输出）----\n",
      "{'id': 1, 'aspect': None, 'opinion': '很好', 'category': '整体', 'polarity': '正面'}\n",
      "\n",
      "---- 张量形态（送进 ClassificationModel 的）----\n",
      "input_ids.shape      : (512,)\n",
      "token_type_ids.shape : (512,)\n",
      "attention_mask.shape : (512,)\n",
      "\n",
      "分类标签 cat_labels : 8 -> 整体\n",
      "情感标签 pol_labels : 0 -> 正面\n",
      "\n",
      "---- 前 40 个 token（含特殊符号）----\n",
      "000  token=[CLS]     token_type=0  attn=1\n",
      "001  token=很         token_type=0  attn=1\n",
      "002  token=好         token_type=0  attn=1\n",
      "003  token=，         token_type=0  attn=1\n",
      "004  token=超         token_type=0  attn=1\n",
      "005  token=值         token_type=0  attn=1\n",
      "006  token=，         token_type=0  attn=1\n",
      "007  token=很         token_type=0  attn=1\n",
      "008  token=好         token_type=0  attn=1\n",
      "009  token=用         token_type=0  attn=1\n",
      "010  token=[SEP]     token_type=0  attn=1\n",
      "011  token=隐         token_type=1  attn=1\n",
      "012  token=式         token_type=1  attn=1\n",
      "013  token=[SEP]     token_type=1  attn=1\n",
      "014  token=很         token_type=1  attn=1\n",
      "015  token=好         token_type=1  attn=1\n",
      "016  token=[SEP]     token_type=1  attn=1\n",
      "017  token=[PAD]     token_type=0  attn=0\n",
      "018  token=[PAD]     token_type=0  attn=0\n",
      "019  token=[PAD]     token_type=0  attn=0\n",
      "020  token=[PAD]     token_type=0  attn=0\n",
      "021  token=[PAD]     token_type=0  attn=0\n",
      "022  token=[PAD]     token_type=0  attn=0\n",
      "023  token=[PAD]     token_type=0  attn=0\n",
      "024  token=[PAD]     token_type=0  attn=0\n",
      "025  token=[PAD]     token_type=0  attn=0\n",
      "026  token=[PAD]     token_type=0  attn=0\n",
      "027  token=[PAD]     token_type=0  attn=0\n",
      "028  token=[PAD]     token_type=0  attn=0\n",
      "029  token=[PAD]     token_type=0  attn=0\n",
      "030  token=[PAD]     token_type=0  attn=0\n",
      "031  token=[PAD]     token_type=0  attn=0\n",
      "032  token=[PAD]     token_type=0  attn=0\n",
      "033  token=[PAD]     token_type=0  attn=0\n",
      "034  token=[PAD]     token_type=0  attn=0\n",
      "035  token=[PAD]     token_type=0  attn=0\n",
      "036  token=[PAD]     token_type=0  attn=0\n",
      "037  token=[PAD]     token_type=0  attn=0\n",
      "038  token=[PAD]     token_type=0  attn=0\n",
      "039  token=[PAD]     token_type=0  attn=0\n",
      "================================================================================\n",
      "样本索引 idx = 1\n",
      "---- 原始 classification 样本（prepare_classification_samples 输出）----\n",
      "{'id': 1, 'aspect': None, 'opinion': '超值', 'category': '价格', 'polarity': '正面'}\n",
      "\n",
      "---- 张量形态（送进 ClassificationModel 的）----\n",
      "input_ids.shape      : (512,)\n",
      "token_type_ids.shape : (512,)\n",
      "attention_mask.shape : (512,)\n",
      "\n",
      "分类标签 cat_labels : 1 -> 价格\n",
      "情感标签 pol_labels : 0 -> 正面\n",
      "\n",
      "---- 前 40 个 token（含特殊符号）----\n",
      "000  token=[CLS]     token_type=0  attn=1\n",
      "001  token=很         token_type=0  attn=1\n",
      "002  token=好         token_type=0  attn=1\n",
      "003  token=，         token_type=0  attn=1\n",
      "004  token=超         token_type=0  attn=1\n",
      "005  token=值         token_type=0  attn=1\n",
      "006  token=，         token_type=0  attn=1\n",
      "007  token=很         token_type=0  attn=1\n",
      "008  token=好         token_type=0  attn=1\n",
      "009  token=用         token_type=0  attn=1\n",
      "010  token=[SEP]     token_type=0  attn=1\n",
      "011  token=隐         token_type=1  attn=1\n",
      "012  token=式         token_type=1  attn=1\n",
      "013  token=[SEP]     token_type=1  attn=1\n",
      "014  token=超         token_type=1  attn=1\n",
      "015  token=值         token_type=1  attn=1\n",
      "016  token=[SEP]     token_type=1  attn=1\n",
      "017  token=[PAD]     token_type=0  attn=0\n",
      "018  token=[PAD]     token_type=0  attn=0\n",
      "019  token=[PAD]     token_type=0  attn=0\n",
      "020  token=[PAD]     token_type=0  attn=0\n",
      "021  token=[PAD]     token_type=0  attn=0\n",
      "022  token=[PAD]     token_type=0  attn=0\n",
      "023  token=[PAD]     token_type=0  attn=0\n",
      "024  token=[PAD]     token_type=0  attn=0\n",
      "025  token=[PAD]     token_type=0  attn=0\n",
      "026  token=[PAD]     token_type=0  attn=0\n",
      "027  token=[PAD]     token_type=0  attn=0\n",
      "028  token=[PAD]     token_type=0  attn=0\n",
      "029  token=[PAD]     token_type=0  attn=0\n",
      "030  token=[PAD]     token_type=0  attn=0\n",
      "031  token=[PAD]     token_type=0  attn=0\n",
      "032  token=[PAD]     token_type=0  attn=0\n",
      "033  token=[PAD]     token_type=0  attn=0\n",
      "034  token=[PAD]     token_type=0  attn=0\n",
      "035  token=[PAD]     token_type=0  attn=0\n",
      "036  token=[PAD]     token_type=0  attn=0\n",
      "037  token=[PAD]     token_type=0  attn=0\n",
      "038  token=[PAD]     token_type=0  attn=0\n",
      "039  token=[PAD]     token_type=0  attn=0\n",
      "================================================================================\n",
      "样本索引 idx = 2\n",
      "---- 原始 classification 样本（prepare_classification_samples 输出）----\n",
      "{'id': 1, 'aspect': None, 'opinion': '很好用', 'category': '整体', 'polarity': '正面'}\n",
      "\n",
      "---- 张量形态（送进 ClassificationModel 的）----\n",
      "input_ids.shape      : (512,)\n",
      "token_type_ids.shape : (512,)\n",
      "attention_mask.shape : (512,)\n",
      "\n",
      "分类标签 cat_labels : 8 -> 整体\n",
      "情感标签 pol_labels : 0 -> 正面\n",
      "\n",
      "---- 前 40 个 token（含特殊符号）----\n",
      "000  token=[CLS]     token_type=0  attn=1\n",
      "001  token=很         token_type=0  attn=1\n",
      "002  token=好         token_type=0  attn=1\n",
      "003  token=，         token_type=0  attn=1\n",
      "004  token=超         token_type=0  attn=1\n",
      "005  token=值         token_type=0  attn=1\n",
      "006  token=，         token_type=0  attn=1\n",
      "007  token=很         token_type=0  attn=1\n",
      "008  token=好         token_type=0  attn=1\n",
      "009  token=用         token_type=0  attn=1\n",
      "010  token=[SEP]     token_type=0  attn=1\n",
      "011  token=隐         token_type=1  attn=1\n",
      "012  token=式         token_type=1  attn=1\n",
      "013  token=[SEP]     token_type=1  attn=1\n",
      "014  token=很         token_type=1  attn=1\n",
      "015  token=好         token_type=1  attn=1\n",
      "016  token=用         token_type=1  attn=1\n",
      "017  token=[SEP]     token_type=1  attn=1\n",
      "018  token=[PAD]     token_type=0  attn=0\n",
      "019  token=[PAD]     token_type=0  attn=0\n",
      "020  token=[PAD]     token_type=0  attn=0\n",
      "021  token=[PAD]     token_type=0  attn=0\n",
      "022  token=[PAD]     token_type=0  attn=0\n",
      "023  token=[PAD]     token_type=0  attn=0\n",
      "024  token=[PAD]     token_type=0  attn=0\n",
      "025  token=[PAD]     token_type=0  attn=0\n",
      "026  token=[PAD]     token_type=0  attn=0\n",
      "027  token=[PAD]     token_type=0  attn=0\n",
      "028  token=[PAD]     token_type=0  attn=0\n",
      "029  token=[PAD]     token_type=0  attn=0\n",
      "030  token=[PAD]     token_type=0  attn=0\n",
      "031  token=[PAD]     token_type=0  attn=0\n",
      "032  token=[PAD]     token_type=0  attn=0\n",
      "033  token=[PAD]     token_type=0  attn=0\n",
      "034  token=[PAD]     token_type=0  attn=0\n",
      "035  token=[PAD]     token_type=0  attn=0\n",
      "036  token=[PAD]     token_type=0  attn=0\n",
      "037  token=[PAD]     token_type=0  attn=0\n",
      "038  token=[PAD]     token_type=0  attn=0\n",
      "039  token=[PAD]     token_type=0  attn=0\n",
      "================================================================================\n",
      "样本索引 idx = 3\n",
      "---- 原始 classification 样本（prepare_classification_samples 输出）----\n",
      "{'id': 2, 'aspect': None, 'opinion': '很好', 'category': '整体', 'polarity': '正面'}\n",
      "\n",
      "---- 张量形态（送进 ClassificationModel 的）----\n",
      "input_ids.shape      : (512,)\n",
      "token_type_ids.shape : (512,)\n",
      "attention_mask.shape : (512,)\n",
      "\n",
      "分类标签 cat_labels : 8 -> 整体\n",
      "情感标签 pol_labels : 0 -> 正面\n",
      "\n",
      "---- 前 40 个 token（含特殊符号）----\n",
      "000  token=[CLS]     token_type=0  attn=1\n",
      "001  token=很         token_type=0  attn=1\n",
      "002  token=好         token_type=0  attn=1\n",
      "003  token=，         token_type=0  attn=1\n",
      "004  token=遮         token_type=0  attn=1\n",
      "005  token=暇         token_type=0  attn=1\n",
      "006  token=功         token_type=0  attn=1\n",
      "007  token=能         token_type=0  attn=1\n",
      "008  token=差         token_type=0  attn=1\n",
      "009  token=一         token_type=0  attn=1\n",
      "010  token=些         token_type=0  attn=1\n",
      "011  token=，         token_type=0  attn=1\n",
      "012  token=总         token_type=0  attn=1\n",
      "013  token=体         token_type=0  attn=1\n",
      "014  token=还         token_type=0  attn=1\n",
      "015  token=不         token_type=0  attn=1\n",
      "016  token=错         token_type=0  attn=1\n",
      "017  token=[SEP]     token_type=0  attn=1\n",
      "018  token=隐         token_type=1  attn=1\n",
      "019  token=式         token_type=1  attn=1\n",
      "020  token=[SEP]     token_type=1  attn=1\n",
      "021  token=很         token_type=1  attn=1\n",
      "022  token=好         token_type=1  attn=1\n",
      "023  token=[SEP]     token_type=1  attn=1\n",
      "024  token=[PAD]     token_type=0  attn=0\n",
      "025  token=[PAD]     token_type=0  attn=0\n",
      "026  token=[PAD]     token_type=0  attn=0\n",
      "027  token=[PAD]     token_type=0  attn=0\n",
      "028  token=[PAD]     token_type=0  attn=0\n",
      "029  token=[PAD]     token_type=0  attn=0\n",
      "030  token=[PAD]     token_type=0  attn=0\n",
      "031  token=[PAD]     token_type=0  attn=0\n",
      "032  token=[PAD]     token_type=0  attn=0\n",
      "033  token=[PAD]     token_type=0  attn=0\n",
      "034  token=[PAD]     token_type=0  attn=0\n",
      "035  token=[PAD]     token_type=0  attn=0\n",
      "036  token=[PAD]     token_type=0  attn=0\n",
      "037  token=[PAD]     token_type=0  attn=0\n",
      "038  token=[PAD]     token_type=0  attn=0\n",
      "039  token=[PAD]     token_type=0  attn=0\n",
      "================================================================================\n",
      "样本索引 idx = 4\n",
      "---- 原始 classification 样本（prepare_classification_samples 输出）----\n",
      "{'id': 2, 'aspect': '遮暇功能', 'opinion': '差一些', 'category': '功效', 'polarity': '负面'}\n",
      "\n",
      "---- 张量形态（送进 ClassificationModel 的）----\n",
      "input_ids.shape      : (512,)\n",
      "token_type_ids.shape : (512,)\n",
      "attention_mask.shape : (512,)\n",
      "\n",
      "分类标签 cat_labels : 4 -> 功效\n",
      "情感标签 pol_labels : 1 -> 负面\n",
      "\n",
      "---- 前 40 个 token（含特殊符号）----\n",
      "000  token=[CLS]     token_type=0  attn=1\n",
      "001  token=很         token_type=0  attn=1\n",
      "002  token=好         token_type=0  attn=1\n",
      "003  token=，         token_type=0  attn=1\n",
      "004  token=遮         token_type=0  attn=1\n",
      "005  token=暇         token_type=0  attn=1\n",
      "006  token=功         token_type=0  attn=1\n",
      "007  token=能         token_type=0  attn=1\n",
      "008  token=差         token_type=0  attn=1\n",
      "009  token=一         token_type=0  attn=1\n",
      "010  token=些         token_type=0  attn=1\n",
      "011  token=，         token_type=0  attn=1\n",
      "012  token=总         token_type=0  attn=1\n",
      "013  token=体         token_type=0  attn=1\n",
      "014  token=还         token_type=0  attn=1\n",
      "015  token=不         token_type=0  attn=1\n",
      "016  token=错         token_type=0  attn=1\n",
      "017  token=[SEP]     token_type=0  attn=1\n",
      "018  token=遮         token_type=1  attn=1\n",
      "019  token=暇         token_type=1  attn=1\n",
      "020  token=功         token_type=1  attn=1\n",
      "021  token=能         token_type=1  attn=1\n",
      "022  token=[SEP]     token_type=1  attn=1\n",
      "023  token=差         token_type=1  attn=1\n",
      "024  token=一         token_type=1  attn=1\n",
      "025  token=些         token_type=1  attn=1\n",
      "026  token=[SEP]     token_type=1  attn=1\n",
      "027  token=[PAD]     token_type=0  attn=0\n",
      "028  token=[PAD]     token_type=0  attn=0\n",
      "029  token=[PAD]     token_type=0  attn=0\n",
      "030  token=[PAD]     token_type=0  attn=0\n",
      "031  token=[PAD]     token_type=0  attn=0\n",
      "032  token=[PAD]     token_type=0  attn=0\n",
      "033  token=[PAD]     token_type=0  attn=0\n",
      "034  token=[PAD]     token_type=0  attn=0\n",
      "035  token=[PAD]     token_type=0  attn=0\n",
      "036  token=[PAD]     token_type=0  attn=0\n",
      "037  token=[PAD]     token_type=0  attn=0\n",
      "038  token=[PAD]     token_type=0  attn=0\n",
      "039  token=[PAD]     token_type=0  attn=0\n"
     ]
    }
   ],
   "source": [
    "## 写一个代码，生成一个classification任务的样本我看看。\n",
    "## 你似乎只打出了一个例子，能多打几个出来吗？\n",
    "import os\n",
    "import torch\n",
    "from transformers import BertTokenizerFast as BertTokenizer\n",
    "\n",
    "from config import Config\n",
    "from utils_loc import load_train_data, ClassificationDataset\n",
    "from train import prepare_classification_samples\n",
    "\n",
    "\n",
    "def show_one(idx: int, samples, dataset, tokenizer, max_tokens: int):\n",
    "    raw_sample = samples[idx]\n",
    "    item = dataset[idx]\n",
    "\n",
    "    input_ids = item[\"input_ids\"]\n",
    "    token_type_ids = item[\"token_type_ids\"]\n",
    "    attention_mask = item[\"attention_mask\"]\n",
    "    cat_labels = item.get(\"cat_labels\")\n",
    "    pol_labels = item.get(\"pol_labels\")\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"样本索引 idx = {idx}\")\n",
    "    print(\"---- 原始 classification 样本（prepare_classification_samples 输出）----\")\n",
    "    print(raw_sample)\n",
    "\n",
    "    print(\"\\n---- 张量形态（送进 ClassificationModel 的）----\")\n",
    "    print(f\"input_ids.shape      : {tuple(input_ids.shape)}\")\n",
    "    print(f\"token_type_ids.shape : {tuple(token_type_ids.shape)}\")\n",
    "    print(f\"attention_mask.shape : {tuple(attention_mask.shape)}\")\n",
    "\n",
    "    if cat_labels is not None and pol_labels is not None:\n",
    "        cat_idx = int(cat_labels.item())\n",
    "        pol_idx = int(pol_labels.item())\n",
    "        cat = Config.IDX2CAT[cat_idx]\n",
    "        pol = Config.IDX2POL[pol_idx]\n",
    "        print(f\"\\n分类标签 cat_labels : {cat_idx} -> {cat}\")\n",
    "        print(f\"情感标签 pol_labels : {pol_idx} -> {pol}\")\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids.tolist())\n",
    "    print(f\"\\n---- 前 {max_tokens} 个 token（含特殊符号）----\")\n",
    "    for i, tok in enumerate(tokens[:max_tokens]):\n",
    "        tt = int(token_type_ids[i].item())\n",
    "        am = int(attention_mask[i].item())\n",
    "        print(f\"{i:03d}  token={tok:8s}  token_type={tt}  attn={am}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    tokenizer = BertTokenizer.from_pretrained(Config.BERT_PATH)\n",
    "\n",
    "    reviews_df, labels_df = load_train_data()\n",
    "    samples = prepare_classification_samples(reviews_df, labels_df)\n",
    "    print(f\"总样本数: {len(samples)}\")\n",
    "\n",
    "    dataset = ClassificationDataset(reviews_df, samples, tokenizer, Config.MAX_LEN)\n",
    "\n",
    "    # 控制要看的起始索引和数量\n",
    "    start_idx = int(os.environ.get(\"START_IDX\", \"0\"))\n",
    "    num_samples = int(os.environ.get(\"NUM_SAMPLES\", \"5\"))\n",
    "    max_tokens = int(os.environ.get(\"MAX_TOKENS\", \"40\"))\n",
    "\n",
    "    end_idx = min(start_idx + num_samples, len(samples))\n",
    "    print(f\"将展示样本索引 [{start_idx}, {end_idx - 1}]\")\n",
    "\n",
    "    for idx in range(start_idx, end_idx):\n",
    "        show_one(idx, samples, dataset, tokenizer, max_tokens)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3504dc-6e61-4302-b11d-3b4ef79df6b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c237fb5f-8e89-429e-b8b9-c5b9462d064b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总 samples: 11393\n",
      "将展示 rid 数量: 5\n",
      "\n",
      "====================================================================================================\n",
      "RID = 2\n",
      "Review = 很好，遮暇功能差一些，总体还不错\n",
      "\n",
      "[正样本] category != 'None'（真实四元组/正例）\n",
      "  (0) aspect=_(隐式)  opinion=很好  category=整体  polarity=正面\n",
      "  (1) aspect=遮暇功能  opinion=差一些  category=功效  polarity=负面\n",
      "  (2) aspect=_(隐式)  opinion=还不错  category=整体  polarity=正面\n",
      "\n",
      "[负样本类型1] 显式A + 显式O 的错配（aspect!=None 且 category=='None'）\n",
      "  (0) aspect=遮暇功能  opinion=很好  category=None  polarity=中性\n",
      "  (1) aspect=遮暇功能  opinion=还不错  category=None  polarity=中性\n",
      "\n",
      "[负样本类型2] 隐式A(None) + 显式O（aspect==None 且 category=='None'）\n",
      "  (0) aspect=_(隐式)  opinion=差一些  category=None  polarity=中性\n",
      "\n",
      "====================================================================================================\n",
      "RID = 3\n",
      "Review = 包装太随便了，连个包装盒都没有，第一感觉很不好\n",
      "\n",
      "[正样本] category != 'None'（真实四元组/正例）\n",
      "  (0) aspect=包装  opinion=太随便了  category=包装  polarity=负面\n",
      "  (1) aspect=包装盒  opinion=没有  category=包装  polarity=负面\n",
      "  (2) aspect=_(隐式)  opinion=很不好  category=整体  polarity=负面\n",
      "\n",
      "[负样本类型1] 显式A + 显式O 的错配（aspect!=None 且 category=='None'）\n",
      "  (0) aspect=包装盒  opinion=很不好  category=None  polarity=中性\n",
      "  (1) aspect=包装盒  opinion=太随便了  category=None  polarity=中性\n",
      "  (2) aspect=包装  opinion=没有  category=None  polarity=中性\n",
      "  (3) aspect=包装  opinion=很不好  category=None  polarity=中性\n",
      "\n",
      "[负样本类型2] 隐式A(None) + 显式O（aspect==None 且 category=='None'）\n",
      "  (0) aspect=_(隐式)  opinion=没有  category=None  polarity=中性\n",
      "  (1) aspect=_(隐式)  opinion=太随便了  category=None  polarity=中性\n",
      "\n",
      "====================================================================================================\n",
      "RID = 6\n",
      "Review = 使用一段时间才来评价，淡淡的香味，喜欢！\n",
      "\n",
      "[正样本] category != 'None'（真实四元组/正例）\n",
      "  (0) aspect=香味  opinion=淡淡的  category=气味  polarity=正面\n",
      "  (1) aspect=_(隐式)  opinion=喜欢  category=整体  polarity=正面\n",
      "\n",
      "[负样本类型1] 显式A + 显式O 的错配（aspect!=None 且 category=='None'）\n",
      "  (0) aspect=香味  opinion=喜欢  category=None  polarity=中性\n",
      "\n",
      "[负样本类型2] 隐式A(None) + 显式O（aspect==None 且 category=='None'）\n",
      "  (0) aspect=_(隐式)  opinion=淡淡的  category=None  polarity=中性\n",
      "\n",
      "====================================================================================================\n",
      "RID = 13\n",
      "Review = 一直在**买东西，质量值得信赖物流还快，以后还会继续关注的。\n",
      "\n",
      "[正样本] category != 'None'（真实四元组/正例）\n",
      "  (0) aspect=物流  opinion=快  category=物流  polarity=正面\n",
      "\n",
      "[负样本类型1] 显式A + 显式O 的错配（aspect!=None 且 category=='None'）\n",
      "  (无)\n",
      "\n",
      "[负样本类型2] 隐式A(None) + 显式O（aspect==None 且 category=='None'）\n",
      "  (0) aspect=_(隐式)  opinion=快  category=None  polarity=中性\n",
      "\n",
      "====================================================================================================\n",
      "RID = 15\n",
      "Review = 上妆效果：容易上妆 个人情况：油性皮肤 持久情况：下午好像有点出油 整体描述：还不错 适合肤质：油性肤质\n",
      "\n",
      "[正样本] category != 'None'（真实四元组/正例）\n",
      "  (0) aspect=上妆效果  opinion=容易上妆  category=功效  polarity=正面\n",
      "  (1) aspect=持久情况  opinion=_  category=功效  polarity=负面\n",
      "  (2) aspect=_(隐式)  opinion=还不错  category=整体  polarity=正面\n",
      "\n",
      "[负样本类型1] 显式A + 显式O 的错配（aspect!=None 且 category=='None'）\n",
      "  (0) aspect=上妆效果  opinion=还不错  category=None  polarity=中性\n",
      "  (1) aspect=上妆效果  opinion=_  category=None  polarity=中性\n",
      "  (2) aspect=持久情况  opinion=容易上妆  category=None  polarity=中性\n",
      "  (3) aspect=持久情况  opinion=还不错  category=None  polarity=中性\n",
      "\n",
      "[负样本类型2] 隐式A(None) + 显式O（aspect==None 且 category=='None'）\n",
      "  (0) aspect=_(隐式)  opinion=容易上妆  category=None  polarity=中性\n",
      "  (1) aspect=_(隐式)  opinion=_  category=None  polarity=中性\n"
     ]
    }
   ],
   "source": [
    "## 两类负样本，都搞点样例给我看看？\n",
    "\n",
    "import os\n",
    "from utils_loc import load_train_data\n",
    "from train import prepare_classification_samples\n",
    "\n",
    "\n",
    "def fmt_asp(a):\n",
    "    return a if a is not None else \"_(隐式)\"\n",
    "\n",
    "\n",
    "def truncate(s, n):\n",
    "    s = \"\" if s is None else str(s)\n",
    "    return s if len(s) <= n else (s[:n] + \"...\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    num_rids = int(os.environ.get(\"NUM_RIDS\", \"5\"))\n",
    "    max_pos = int(os.environ.get(\"MAX_POS\", \"8\"))\n",
    "    max_neg1 = int(os.environ.get(\"MAX_NEG1\", \"6\"))\n",
    "    max_neg2 = int(os.environ.get(\"MAX_NEG2\", \"6\"))\n",
    "    max_text = int(os.environ.get(\"MAX_TEXT\", \"120\"))\n",
    "    only_rid = os.environ.get(\"RID\", \"\").strip()\n",
    "\n",
    "    reviews_df, labels_df = load_train_data()\n",
    "    review_map = reviews_df.set_index(\"id\")[\"Reviews\"].astype(str).to_dict()\n",
    "\n",
    "    samples = prepare_classification_samples(reviews_df, labels_df)\n",
    "\n",
    "    by_rid = {}\n",
    "    for s in samples:\n",
    "        rid = int(s[\"id\"])\n",
    "        by_rid.setdefault(rid, []).append(s)\n",
    "\n",
    "    if only_rid:\n",
    "        rid_list = [int(only_rid)]\n",
    "    else:\n",
    "        candidates = []\n",
    "        for rid, arr in by_rid.items():\n",
    "            has_pos = any(x.get(\"category\") != \"None\" for x in arr)\n",
    "            has_neg = any(x.get(\"category\") == \"None\" for x in arr)\n",
    "            if has_pos and has_neg:\n",
    "                candidates.append(rid)\n",
    "        rid_list = sorted(candidates)[:num_rids]\n",
    "\n",
    "    print(f\"总 samples: {len(samples)}\")\n",
    "    print(f\"将展示 rid 数量: {len(rid_list)}\")\n",
    "\n",
    "    for rid in rid_list:\n",
    "        arr = by_rid.get(rid, [])\n",
    "        pos = [x for x in arr if x.get(\"category\") != \"None\"]\n",
    "        neg1 = [x for x in arr if x.get(\"category\") == \"None\" and x.get(\"aspect\") is not None]\n",
    "        neg2 = [x for x in arr if x.get(\"category\") == \"None\" and x.get(\"aspect\") is None]\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 100)\n",
    "        print(f\"RID = {rid}\")\n",
    "        print(f\"Review = {truncate(review_map.get(rid, ''), max_text)}\")\n",
    "\n",
    "        print(\"\\n[正样本] category != 'None'（真实四元组/正例）\")\n",
    "        if not pos:\n",
    "            print(\"  (无)\")\n",
    "        else:\n",
    "            for i, s in enumerate(pos[:max_pos]):\n",
    "                print(\n",
    "                    f\"  ({i}) aspect={fmt_asp(s['aspect'])}  opinion={s['opinion']}  \"\n",
    "                    f\"category={s['category']}  polarity={s['polarity']}\"\n",
    "                )\n",
    "\n",
    "        print(\"\\n[负样本类型1] 显式A + 显式O 的错配（aspect!=None 且 category=='None'）\")\n",
    "        if not neg1:\n",
    "            print(\"  (无)\")\n",
    "        else:\n",
    "            for i, s in enumerate(neg1[:max_neg1]):\n",
    "                print(\n",
    "                    f\"  ({i}) aspect={fmt_asp(s['aspect'])}  opinion={s['opinion']}  \"\n",
    "                    f\"category={s['category']}  polarity={s['polarity']}\"\n",
    "                )\n",
    "\n",
    "        print(\"\\n[负样本类型2] 隐式A(None) + 显式O（aspect==None 且 category=='None'）\")\n",
    "        if not neg2:\n",
    "            print(\"  (无)\")\n",
    "        else:\n",
    "            for i, s in enumerate(neg2[:max_neg2]):\n",
    "                print(\n",
    "                    f\"  ({i}) aspect={fmt_asp(s['aspect'])}  opinion={s['opinion']}  \"\n",
    "                    f\"category={s['category']}  polarity={s['polarity']}\"\n",
    "                )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eb5a6d96-235d-486f-b1e1-d97c4ceae6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id,AspectTerms,A_start,A_end,OpinionTerms,O_start,O_end,Categories,Polarities\n",
      "1,_, , ,很好,0,2,整体,正面\n",
      "1,_, , ,超值,3,5,价格,正面\n",
      "1,_, , ,很好用,6,9,整体,正面\n",
      "2,_, , ,很好,0,2,整体,正面\n",
      "2,遮暇功能,3,7,差一些,7,10,功效,负面\n",
      "2,_, , ,还不错,13,16,整体,正面\n",
      "3,包装,0,2,太随便了,2,6,包装,负面\n",
      "3,包装盒,9,12,没有,13,15,包装,负面\n",
      "3,_, , ,很不好,20,23,整体,负面\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!head /mnt/d/forCoding_data/Tianchi_Ecommerce6/TRAIN/Train_labels.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d45ce2-0679-4356-94a7-b904fb51f78c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f0bd91d3-9956-403a-a035-3a4badac0118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id,Reviews\n",
      "1,很好，超值，很好用\n",
      "2,很好，遮暇功能差一些，总体还不错\n",
      "3,包装太随便了，连个包装盒都没有，第一感觉很不好\n",
      "4,宝贝收到了，产品非常的不好，简直就是个垃圾，我都扔了。\n",
      "5,活动价很是划算，买一送一共60片才花八十五块，天天用都不心疼啊\n",
      "6,使用一段时间才来评价，淡淡的香味，喜欢！\n",
      "7,之前买过。用了还不错。继续回购\n",
      "8,很润相对好推开。总体还可以。\n",
      "9,收到了，当天晚上就用了很不错哦，\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!head /mnt/d/forCoding_data/Tianchi_Ecommerce6/TRAIN/Train_reviews.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5fd143-aca6-4564-9122-2cbf6fa2102b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d483c5e8-7e17-4989-a081-ba1c606526a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad209309-9f65-4885-a92d-11b7d3b9c754",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e13100e-5862-4361-95cd-6a676108760c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72675a35-bfb3-4c94-99ae-087e1c7bd199",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c79d00-5b0d-4076-8ab0-06c549538d76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a12359-688b-4db4-957a-4c053f6c5622",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e825005-09ce-4cc2-9de3-05cd01db1b18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e64bbba-2b03-4a7d-b7b7-5eab17480298",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791b8160-515f-4826-b989-5440ed875c60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becf6aaa-098d-4d16-8cc9-00a5429a8724",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f6666a-0e19-407d-b791-ce426242c233",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5088690c-8055-4ba5-a09c-bc3762c8dd7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab9e517-ecd4-408e-9c3a-f00865d11fe9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b6570e-6e13-41da-b9c8-b4ccd749de39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b651fc2-7379-4281-8c3a-51f166b8922d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3ac0d7-2f95-4025-ad43-e7d1151cbe6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69093670-f1cc-4cf6-8f03-df2757d3c7e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9370801-9a95-4411-9d85-3f74d0d5bd4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c88221a-4c4d-4042-b7ca-58b4176e06f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiuminke/miniconda3/envs/ml12/lib/python3.11/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "/home/xiuminke/miniconda3/envs/ml12/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading tokenizer and dataset ...\n",
      "dataset size: 3229\n",
      "id: 1\n",
      "text: 很好，超值，很好用\n",
      "input_ids.shape: (512,)\n",
      "attention_mask.sum: 11\n",
      "labels.shape: (512,)\n",
      "001  token=很  span=(0,1)  text='很'  tag=B-OP\n",
      "002  token=好  span=(1,2)  text='好'  tag=I-OP\n",
      "003  token=，  span=(2,3)  text='，'  tag=O\n",
      "004  token=超  span=(3,4)  text='超'  tag=B-OP\n",
      "005  token=值  span=(4,5)  text='值'  tag=I-OP\n",
      "006  token=，  span=(5,6)  text='，'  tag=O\n",
      "007  token=很  span=(6,7)  text='很'  tag=B-OP\n",
      "008  token=好  span=(7,8)  text='好'  tag=I-OP\n",
      "009  token=用  span=(8,9)  text='用'  tag=I-OP\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "from transformers import BertTokenizerFast as BertTokenizer\n",
    "from config import Config\n",
    "from utils_loc import load_train_data, ExtractionDataset\n",
    "\n",
    "def show_sample(dataset, idx=0, limit=30):\n",
    "    item, text, rid, offset_mapping = dataset[idx]\n",
    "    input_ids = item[\"input_ids\"]\n",
    "    attention_mask = item[\"attention_mask\"]\n",
    "    token_type_ids = item[\"token_type_ids\"]\n",
    "    labels = item.get(\"labels\")\n",
    "\n",
    "    print(f\"id: {rid}\")\n",
    "    print(f\"text: {text}\")\n",
    "    print(f\"input_ids.shape: {tuple(input_ids.shape)}\")\n",
    "    print(f\"attention_mask.sum: {int(attention_mask.sum().item())}\")\n",
    "    if labels is not None:\n",
    "        print(f\"labels.shape: {tuple(labels.shape)}\")\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained(Config.BERT_PATH)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids.tolist())\n",
    "\n",
    "    cnt = 0\n",
    "    for i in range(len(tokens)):\n",
    "        if attention_mask[i].item() != 1:\n",
    "            continue\n",
    "        start, end = offset_mapping[i].tolist()\n",
    "        if start == 0 and end == 0:\n",
    "            continue\n",
    "        tok = tokens[i]\n",
    "        tag = None\n",
    "        if labels is not None:\n",
    "            tag_idx = int(labels[i].item())\n",
    "            tag = Config.IDX2TAG.get(tag_idx, str(tag_idx))\n",
    "        line = f\"{i:03d}  token={tok}  span=({start},{end})  text='{text[start:end]}'\"\n",
    "        if tag is not None:\n",
    "            line += f\"  tag={tag}\"\n",
    "        print(line)\n",
    "        cnt += 1\n",
    "        if cnt >= limit:\n",
    "            break\n",
    "\n",
    "# def main():\n",
    "idx = int(os.environ.get(\"IDX\", \"0\"))\n",
    "limit = int(os.environ.get(\"LIMIT\", \"30\"))\n",
    "\n",
    "print(\"loading tokenizer and dataset ...\")\n",
    "tokenizer = BertTokenizer.from_pretrained(Config.BERT_PATH)\n",
    "reviews, labels = load_train_data()\n",
    "dataset = ExtractionDataset(reviews, labels, tokenizer, Config.MAX_LEN)\n",
    "print(f\"dataset size: {len(dataset)}\")\n",
    "\n",
    "show_sample(dataset, idx=idx, limit=limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5249c9a8-3710-431c-a54e-0e0c4a703e2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>很好，超值，很好用</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>很好，遮暇功能差一些，总体还不错</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>包装太随便了，连个包装盒都没有，第一感觉很不好</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>宝贝收到了，产品非常的不好，简直就是个垃圾，我都扔了。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>活动价很是划算，买一送一共60片才花八十五块，天天用都不心疼啊</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3224</th>\n",
       "      <td>3225</td>\n",
       "      <td>***，大品牌值得信赖，皮肤亮白！服务周到！</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3225</th>\n",
       "      <td>3226</td>\n",
       "      <td>之前也有买过，这是并不是第一次购买，应该是之前一样吧，价格挺优惠的</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3226</th>\n",
       "      <td>3227</td>\n",
       "      <td>物流很快，包装不错，第一次使用这个牌子，期待&amp;hellip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3227</th>\n",
       "      <td>3228</td>\n",
       "      <td>感觉还可以，唯一的遗憾就是面膜放了香精，希望***官方能把香精去掉就完美了</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3228</th>\n",
       "      <td>3229</td>\n",
       "      <td>这面膜还挺好用的，连续用了两片补水棒棒的，最主要的是蚕丝面料，轻薄透气。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3229 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                Reviews\n",
       "0        1                              很好，超值，很好用\n",
       "1        2                       很好，遮暇功能差一些，总体还不错\n",
       "2        3                包装太随便了，连个包装盒都没有，第一感觉很不好\n",
       "3        4            宝贝收到了，产品非常的不好，简直就是个垃圾，我都扔了。\n",
       "4        5        活动价很是划算，买一送一共60片才花八十五块，天天用都不心疼啊\n",
       "...    ...                                    ...\n",
       "3224  3225                 ***，大品牌值得信赖，皮肤亮白！服务周到！\n",
       "3225  3226      之前也有买过，这是并不是第一次购买，应该是之前一样吧，价格挺优惠的\n",
       "3226  3227          物流很快，包装不错，第一次使用这个牌子，期待&hellip\n",
       "3227  3228  感觉还可以，唯一的遗憾就是面膜放了香精，希望***官方能把香精去掉就完美了\n",
       "3228  3229   这面膜还挺好用的，连续用了两片补水棒棒的，最主要的是蚕丝面料，轻薄透气。\n",
       "\n",
       "[3229 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7783edd-6550-4bd3-aaed-1574043d5756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>AspectTerms</th>\n",
       "      <th>A_start</th>\n",
       "      <th>A_end</th>\n",
       "      <th>OpinionTerms</th>\n",
       "      <th>O_start</th>\n",
       "      <th>O_end</th>\n",
       "      <th>Categories</th>\n",
       "      <th>Polarities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>14</td>\n",
       "      <td>_</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>很好的</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>整体</td>\n",
       "      <td>正面</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>14</td>\n",
       "      <td>_</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>不错</td>\n",
       "      <td>28</td>\n",
       "      <td>30</td>\n",
       "      <td>整体</td>\n",
       "      <td>正面</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id AspectTerms A_start A_end OpinionTerms O_start O_end Categories  \\\n",
       "27  14           _                        很好的       0     3         整体   \n",
       "28  14           _                         不错      28    30         整体   \n",
       "\n",
       "   Polarities  \n",
       "27         正面  \n",
       "28         正面  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[labels.id==14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ebcad14-0796-43d5-9855-b2ad08d7eb7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>AspectTerms</th>\n",
       "      <th>A_start</th>\n",
       "      <th>A_end</th>\n",
       "      <th>OpinionTerms</th>\n",
       "      <th>O_start</th>\n",
       "      <th>O_end</th>\n",
       "      <th>Categories</th>\n",
       "      <th>Polarities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>遮暇功能</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>差一些</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>功效</td>\n",
       "      <td>负面</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>包装</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>太随便了</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>包装</td>\n",
       "      <td>负面</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>包装盒</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>没有</td>\n",
       "      <td>13</td>\n",
       "      <td>15</td>\n",
       "      <td>包装</td>\n",
       "      <td>负面</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>6</td>\n",
       "      <td>香味</td>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "      <td>淡淡的</td>\n",
       "      <td>11</td>\n",
       "      <td>14</td>\n",
       "      <td>气味</td>\n",
       "      <td>正面</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>13</td>\n",
       "      <td>物流</td>\n",
       "      <td>15</td>\n",
       "      <td>17</td>\n",
       "      <td>快</td>\n",
       "      <td>18</td>\n",
       "      <td>19</td>\n",
       "      <td>物流</td>\n",
       "      <td>正面</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6624</th>\n",
       "      <td>3227</td>\n",
       "      <td>物流</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>很快</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>物流</td>\n",
       "      <td>正面</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6625</th>\n",
       "      <td>3227</td>\n",
       "      <td>包装</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>不错</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>包装</td>\n",
       "      <td>正面</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6626</th>\n",
       "      <td>3228</td>\n",
       "      <td>香精</td>\n",
       "      <td>17</td>\n",
       "      <td>19</td>\n",
       "      <td>_</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>成分</td>\n",
       "      <td>负面</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6629</th>\n",
       "      <td>3229</td>\n",
       "      <td>补水</td>\n",
       "      <td>15</td>\n",
       "      <td>17</td>\n",
       "      <td>棒棒的</td>\n",
       "      <td>17</td>\n",
       "      <td>20</td>\n",
       "      <td>功效</td>\n",
       "      <td>正面</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6630</th>\n",
       "      <td>3229</td>\n",
       "      <td>蚕丝面料</td>\n",
       "      <td>26</td>\n",
       "      <td>30</td>\n",
       "      <td>_</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>成分</td>\n",
       "      <td>正面</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1898 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id AspectTerms A_start A_end OpinionTerms O_start O_end Categories  \\\n",
       "4        2        遮暇功能       3     7          差一些       7    10         功效   \n",
       "6        3          包装       0     2         太随便了       2     6         包装   \n",
       "7        3         包装盒       9    12           没有      13    15         包装   \n",
       "12       6          香味      14    16          淡淡的      11    14         气味   \n",
       "26      13          物流      15    17            快      18    19         物流   \n",
       "...    ...         ...     ...   ...          ...     ...   ...        ...   \n",
       "6624  3227          物流       0     2           很快       2     4         物流   \n",
       "6625  3227          包装       5     7           不错       7     9         包装   \n",
       "6626  3228          香精      17    19            _                       成分   \n",
       "6629  3229          补水      15    17          棒棒的      17    20         功效   \n",
       "6630  3229        蚕丝面料      26    30            _                       成分   \n",
       "\n",
       "     Polarities  \n",
       "4            负面  \n",
       "6            负面  \n",
       "7            负面  \n",
       "12           正面  \n",
       "26           正面  \n",
       "...         ...  \n",
       "6624         正面  \n",
       "6625         正面  \n",
       "6626         负面  \n",
       "6629         正面  \n",
       "6630         正面  \n",
       "\n",
       "[1898 rows x 9 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[labels.AspectTerms!=\"_\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed7d1003-3d0b-452b-9b36-b2760869020d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Categories\n",
       "整体      2822\n",
       "使用体验    1042\n",
       "功效       726\n",
       "价格       696\n",
       "物流       517\n",
       "气味       225\n",
       "包装       195\n",
       "真伪       161\n",
       "服务        86\n",
       "其他        65\n",
       "成分        61\n",
       "尺寸        24\n",
       "新鲜度       13\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.Categories.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375d583f-70ba-4ca8-bacb-55a9eeb92bf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e8ace4-f523-45ea-a5d0-bdb5d077869f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724d6c93-2d96-4041-ba0f-2b4b1fd6ef6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd46ebca-bc30-4ff9-974f-7f1b16438a27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8ef2cd-d5f6-4bd3-8022-6a586e292835",
   "metadata": {},
   "outputs": [],
   "source": [
    "({'input_ids': tensor([ 101, 2523, 1962, 8024, 6631,  966, 8024, 2523, 1962, 4500,  102,    0,\n",
    "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
    "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
    "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
    "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
    "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
    "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
    "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
    "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
    "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
    "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
    "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
    "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
    "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
    "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
    "、]),\n",
    "  'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "]),\n",
    "  'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "]),\n",
    "  'labels': tensor([0, 3, 4, 0, 3, 4, 0, 3, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "])},\n",
    " '很好，超值，很好用',\n",
    " 1,\n",
    " tensor([[0, 0],\n",
    "         [0, 1],\n",
    "         [1, 2],\n",
    "         ...,\n",
    "         [0, 0],\n",
    "         [0, 0],\n",
    "         [0, 0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500faef3-ff53-4bdb-952a-74e8046f15d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "252e267b-f1f7-4388-98cd-1e368a6ffa26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'input_ids': tensor([ 101, 2523, 1962, 8024, 6902, 3259, 1216, 5543, 2345,  671,  763, 8024,\n",
       "          2600,  860, 6820,  679, 7231,  102,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0]),\n",
       "  'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "  'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "  'labels': tensor([0, 3, 4, 0, 1, 2, 2, 2, 3, 4, 4, 0, 0, 0, 3, 4, 4, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0])},\n",
       " '很好，遮暇功能差一些，总体还不错',\n",
       " 2,\n",
       " tensor([[0, 0],\n",
       "         [0, 1],\n",
       "         [1, 2],\n",
       "         ...,\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfce3ee-773d-4e01-b445-6a6e5a78859f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31611b57-cb55-4b85-bd99-fc5fe62ffbba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
